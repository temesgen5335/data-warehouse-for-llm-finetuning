{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data from mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.database import MongoDB\n",
    "\n",
    "# Connect to MongoDB\n",
    "db_name = 'clean_data'\n",
    "collection_name = 'alain_news_clean'\n",
    "connection_string = 'mongodb://localhost:27017/'\n",
    "\n",
    "clean_db = MongoDB(db_name=db_name, collection_name=collection_name, connection_string=connection_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_db.remove_duplicates(field='article_url', collection_name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into a pandas DataFrame\n",
    "df = pd.DataFrame(list(clean_db.collection.find()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>image_url</th>\n",
       "      <th>title</th>\n",
       "      <th>article_url</th>\n",
       "      <th>category</th>\n",
       "      <th>author</th>\n",
       "      <th>summary</th>\n",
       "      <th>content</th>\n",
       "      <th>source</th>\n",
       "      <th>published_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6657afc42d8a020654183dab</td>\n",
       "      <td>https://cdn.al-ain.com/sm/images/2024/4/15/252...</td>\n",
       "      <td>ኢራን በእስራኤል ላይ የፈፀመችውን ጥቃት ተከትሎ የአፍሪካ ሀገራት ምን አሉ</td>\n",
       "      <td>https://am.al-ain.com/article/african-nations-...</td>\n",
       "      <td>politics</td>\n",
       "      <td>አል-ዐይን</td>\n",
       "      <td>በርካታ የአፍሪካ ሀገራት ሁለቱ ሀገራት ውጥረትን ከሚያባብሱ ተግባራት እን...</td>\n",
       "      <td>በርካታ የአፍሪካ ሀገራት ሁለቱ ሀገራት ውጥረትን ከሚያባብሱ ተግባራት እን...</td>\n",
       "      <td>https://am.al-ain.com/</td>\n",
       "      <td>2024/4/15 10:20 GMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6657afc52d8a020654183dac</td>\n",
       "      <td>https://cdn.al-ain.com/sm/images/2024/4/15/273...</td>\n",
       "      <td>አሜሪካ እስራኤል በኢራን ላይ በምትወስደው የአፃፋ እርምጃ እጄን አላስገባ...</td>\n",
       "      <td>https://am.al-ain.com/article/us-israeli-retal...</td>\n",
       "      <td>politics</td>\n",
       "      <td>አል-ዐይን</td>\n",
       "      <td>የእስራኤል የጦር ካቢኔ በኢራን ላይ እርምጃ እንዲወሰድ ተስማምቷል</td>\n",
       "      <td>የእስራኤል የጦር ካቢኔ በኢራን ላይ እርምጃ እንዲወሰድ ተስማምቷል\\nአሜሪ...</td>\n",
       "      <td>https://am.al-ain.com/</td>\n",
       "      <td>2024/4/15 7:21 GMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6657afc62d8a020654183dad</td>\n",
       "      <td>https://cdn.al-ain.com/sm/images/2024/4/15/243...</td>\n",
       "      <td>ኢራን በእስራኤል ላይ ጥቃት ካደረሰች በኋላ በመካከለኛው ምስራቅ የበረራ ...</td>\n",
       "      <td>https://am.al-ain.com/article/iran-attack-caus...</td>\n",
       "      <td>politics</td>\n",
       "      <td>አል-ዐይን</td>\n",
       "      <td>ባለፉት ሁለት ቀናት ቢያንስ ከ የሚሆኑ አየር መንገዶች በረራዎችን ሰርዘዋ...</td>\n",
       "      <td>ባለፉት ሁለት ቀናት ቢያንስ ከ የሚሆኑ አየር መንገዶች በረራዎችን ሰርዘዋ...</td>\n",
       "      <td>https://am.al-ain.com/</td>\n",
       "      <td>2024/4/15 6:59 GMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6657afc82d8a020654183dae</td>\n",
       "      <td>https://cdn.al-ain.com/sm/images/2024/4/14/252...</td>\n",
       "      <td>ኢራን በእስራኤል ላይ የፈፀመችውን ጥቃት ተከትሎ ሀገራት ምን አሉ</td>\n",
       "      <td>https://am.al-ain.com/article/iran-israel-atta...</td>\n",
       "      <td>politics</td>\n",
       "      <td>አል-ዐይን</td>\n",
       "      <td>የተመድ ዋና ፀሀፊ አለም ሌላ ተጨማሪ ጦርነት ማስተናገድ አትችልም ብለዋል</td>\n",
       "      <td>የተመድ ዋና ፀሀፊ አለም ሌላ ተጨማሪ ጦርነት ማስተናገድ አትችልም ብለዋል...</td>\n",
       "      <td>https://am.al-ain.com/</td>\n",
       "      <td>2024/4/14 15:19 GMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6657afc92d8a020654183daf</td>\n",
       "      <td>https://cdn.al-ain.com/sm/images/2024/4/14/258...</td>\n",
       "      <td>አሜሪካ ጦሯን ከኒጀር እንድታስወጣ ተጠየቀች</td>\n",
       "      <td>https://am.al-ain.com/article/us-requested-to-...</td>\n",
       "      <td>politics</td>\n",
       "      <td>አል-ዐይን</td>\n",
       "      <td>የሩሲያ ቅጥረኛ ወታደሮች ከሰሞኑ ወደ ኒያሚ ማምራታቸው ይታወሳል</td>\n",
       "      <td>የሩሲያ ቅጥረኛ ወታደሮች ከሰሞኑ ወደ ኒያሚ ማምራታቸው ይታወሳል\\nአሜሪካ...</td>\n",
       "      <td>https://am.al-ain.com/</td>\n",
       "      <td>2024/4/14 14:24 GMT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id  \\\n",
       "0  6657afc42d8a020654183dab   \n",
       "1  6657afc52d8a020654183dac   \n",
       "2  6657afc62d8a020654183dad   \n",
       "3  6657afc82d8a020654183dae   \n",
       "4  6657afc92d8a020654183daf   \n",
       "\n",
       "                                           image_url  \\\n",
       "0  https://cdn.al-ain.com/sm/images/2024/4/15/252...   \n",
       "1  https://cdn.al-ain.com/sm/images/2024/4/15/273...   \n",
       "2  https://cdn.al-ain.com/sm/images/2024/4/15/243...   \n",
       "3  https://cdn.al-ain.com/sm/images/2024/4/14/252...   \n",
       "4  https://cdn.al-ain.com/sm/images/2024/4/14/258...   \n",
       "\n",
       "                                               title  \\\n",
       "0    ኢራን በእስራኤል ላይ የፈፀመችውን ጥቃት ተከትሎ የአፍሪካ ሀገራት ምን አሉ   \n",
       "1  አሜሪካ እስራኤል በኢራን ላይ በምትወስደው የአፃፋ እርምጃ እጄን አላስገባ...   \n",
       "2  ኢራን በእስራኤል ላይ ጥቃት ካደረሰች በኋላ በመካከለኛው ምስራቅ የበረራ ...   \n",
       "3          ኢራን በእስራኤል ላይ የፈፀመችውን ጥቃት ተከትሎ ሀገራት ምን አሉ   \n",
       "4                        አሜሪካ ጦሯን ከኒጀር እንድታስወጣ ተጠየቀች   \n",
       "\n",
       "                                         article_url  category  author  \\\n",
       "0  https://am.al-ain.com/article/african-nations-...  politics  አል-ዐይን   \n",
       "1  https://am.al-ain.com/article/us-israeli-retal...  politics  አል-ዐይን   \n",
       "2  https://am.al-ain.com/article/iran-attack-caus...  politics  አል-ዐይን   \n",
       "3  https://am.al-ain.com/article/iran-israel-atta...  politics  አል-ዐይን   \n",
       "4  https://am.al-ain.com/article/us-requested-to-...  politics  አል-ዐይን   \n",
       "\n",
       "                                             summary  \\\n",
       "0  በርካታ የአፍሪካ ሀገራት ሁለቱ ሀገራት ውጥረትን ከሚያባብሱ ተግባራት እን...   \n",
       "1          የእስራኤል የጦር ካቢኔ በኢራን ላይ እርምጃ እንዲወሰድ ተስማምቷል   \n",
       "2  ባለፉት ሁለት ቀናት ቢያንስ ከ የሚሆኑ አየር መንገዶች በረራዎችን ሰርዘዋ...   \n",
       "3     የተመድ ዋና ፀሀፊ አለም ሌላ ተጨማሪ ጦርነት ማስተናገድ አትችልም ብለዋል   \n",
       "4           የሩሲያ ቅጥረኛ ወታደሮች ከሰሞኑ ወደ ኒያሚ ማምራታቸው ይታወሳል   \n",
       "\n",
       "                                             content                  source  \\\n",
       "0  በርካታ የአፍሪካ ሀገራት ሁለቱ ሀገራት ውጥረትን ከሚያባብሱ ተግባራት እን...  https://am.al-ain.com/   \n",
       "1  የእስራኤል የጦር ካቢኔ በኢራን ላይ እርምጃ እንዲወሰድ ተስማምቷል\\nአሜሪ...  https://am.al-ain.com/   \n",
       "2  ባለፉት ሁለት ቀናት ቢያንስ ከ የሚሆኑ አየር መንገዶች በረራዎችን ሰርዘዋ...  https://am.al-ain.com/   \n",
       "3  የተመድ ዋና ፀሀፊ አለም ሌላ ተጨማሪ ጦርነት ማስተናገድ አትችልም ብለዋል...  https://am.al-ain.com/   \n",
       "4  የሩሲያ ቅጥረኛ ወታደሮች ከሰሞኑ ወደ ኒያሚ ማምራታቸው ይታወሳል\\nአሜሪካ...  https://am.al-ain.com/   \n",
       "\n",
       "        published_date  \n",
       "0  2024/4/15 10:20 GMT  \n",
       "1   2024/4/15 7:21 GMT  \n",
       "2   2024/4/15 6:59 GMT  \n",
       "3  2024/4/14 15:19 GMT  \n",
       "4  2024/4/14 14:24 GMT  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert your DataFrame into a Hugging Face Dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Given the `data` is the MongoDB data\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# remove the unnecessary columns\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_url\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle_url\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublished_date\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      6\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Given the `data` is the MongoDB data\n",
    "# remove the unnecessary columns\n",
    "df = df.drop(columns=['_id', 'image_url', 'article_url', 'published_date', 'author', 'source', 'summary'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the DataFrame to a Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'category', 'content'],\n",
       "    num_rows: 7856\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into a train and test set\n",
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'category', 'content'],\n",
       "        num_rows: 6284\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'category', 'content'],\n",
       "        num_rows: 1572\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=training_text.txt --model_prefix=m --vocab_size=2000\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: training_text.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: training_text.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 78709 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=7545931\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9523% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=193\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999523\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 78582 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=3797778\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 157619 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 78582\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 125158\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 125158 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=75803 obj=12.1196 num_tokens=236501 num_tokens/piece=3.11994\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=64776 obj=10.5944 num_tokens=238536 num_tokens/piece=3.68247\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=48579 obj=10.5797 num_tokens=254727 num_tokens/piece=5.24356\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=48538 obj=10.5504 num_tokens=254974 num_tokens/piece=5.25308\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=36402 obj=10.671 num_tokens=278291 num_tokens/piece=7.64494\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=36401 obj=10.6362 num_tokens=278328 num_tokens/piece=7.64616\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=27300 obj=10.8156 num_tokens=302973 num_tokens/piece=11.0979\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=27300 obj=10.7712 num_tokens=302981 num_tokens/piece=11.0982\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=20475 obj=11.0057 num_tokens=327534 num_tokens/piece=15.9968\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=20475 obj=10.9517 num_tokens=327535 num_tokens/piece=15.9968\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=15356 obj=11.245 num_tokens=351669 num_tokens/piece=22.9011\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=15356 obj=11.1808 num_tokens=351686 num_tokens/piece=22.9022\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=11517 obj=11.5196 num_tokens=376101 num_tokens/piece=32.6562\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=11517 obj=11.4462 num_tokens=376119 num_tokens/piece=32.6577\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8637 obj=11.8405 num_tokens=398854 num_tokens/piece=46.1797\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8637 obj=11.7556 num_tokens=398866 num_tokens/piece=46.1811\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6477 obj=12.2013 num_tokens=421341 num_tokens/piece=65.0519\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6477 obj=12.1104 num_tokens=421347 num_tokens/piece=65.0528\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4857 obj=12.601 num_tokens=445228 num_tokens/piece=91.6673\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4857 obj=12.4999 num_tokens=445237 num_tokens/piece=91.6691\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3642 obj=13.0426 num_tokens=466688 num_tokens/piece=128.141\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3642 obj=12.9367 num_tokens=466692 num_tokens/piece=128.142\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2731 obj=13.5148 num_tokens=489160 num_tokens/piece=179.114\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2731 obj=13.4003 num_tokens=489165 num_tokens/piece=179.116\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2200 obj=13.8527 num_tokens=505802 num_tokens/piece=229.91\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2200 obj=13.7642 num_tokens=505803 num_tokens/piece=229.91\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Concatenate all the text in your training set into a single file\n",
    "with open('training_text.txt', 'w') as f:\n",
    "  for example in dataset['train']:\n",
    "    f.write(example['title'] + ' ' + example['summary'] + ' ' + example['content'] + '\\n')\n",
    "\n",
    "# Train the SentencePiece model on this file\n",
    "spm.SentencePieceTrainer.train('--input=training_text.txt --model_prefix=m --vocab_size=2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0d57afc1d44e4f9bbe835fcdbbecaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6284 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "396e50fe081b4fa09495351b6e64cbb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1572 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Load the trained SentencePiece model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "\n",
    "def tokenize_function(batch_of_articles):\n",
    "  batch_of_texts = [title + ' ' + summary + ' ' + content \n",
    "            for title, summary, content \n",
    "            in zip(batch_of_articles['title'], batch_of_articles['summary'], batch_of_articles['content'])]\n",
    "  return {'input_ids': [sp.encode_as_ids(text) for text in batch_of_texts]}\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/home/hillary_kipkemoi/hugging_face_models/garri/llama-2-amharic-3784m does not appear to have a file named config.json. Checkout 'https://huggingface.co//home/hillary_kipkemoi/hugging_face_models/garri/llama-2-amharic-3784m/tree/main' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m cache_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# The commit hash is needed, because the model repo was rearranged after this commit (files -> finetuned/files),\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# and I couldn't load the model from the new structure\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Load the pre-trained model\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m llama_model \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/hillary_kipkemoi/hugging_face_models/garri/llama-2-amharic-3784m\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Resize the model's token embeddings\u001b[39;00m\n\u001b[1;32m     17\u001b[0m llama_model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(\u001b[38;5;28mlen\u001b[39m(sp))\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/transformers/modeling_utils.py:3158\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   3157\u001b[0m     config_path \u001b[38;5;241m=\u001b[39m config \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[0;32m-> 3158\u001b[0m     config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3168\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3169\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_from_auto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_auto_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3170\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3171\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3174\u001b[0m     \u001b[38;5;66;03m# In case one passes a config to `from_pretrained` + \"attn_implementation\"\u001b[39;00m\n\u001b[1;32m   3175\u001b[0m     \u001b[38;5;66;03m# override the `_attn_implementation` attribute to `attn_implementation` of the kwargs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3179\u001b[0m     \u001b[38;5;66;03m# we pop attn_implementation from the kwargs but this handles the case where users\u001b[39;00m\n\u001b[1;32m   3180\u001b[0m     \u001b[38;5;66;03m# passes manually the config to `from_pretrained`.\u001b[39;00m\n\u001b[1;32m   3181\u001b[0m     config \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(config)\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/transformers/configuration_utils.py:603\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m revision\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_set_token_in_kwargs(kwargs, token)\n\u001b[0;32m--> 603\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type:\n\u001b[1;32m    605\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    606\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are using a model of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to instantiate a model of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    607\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    608\u001b[0m     )\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/transformers/configuration_utils.py:632\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 632\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    634\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/transformers/configuration_utils.py:689\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    685\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:370\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(resolved_file):\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[0;32m--> 370\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    371\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Checkout \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    373\u001b[0m         )\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: /home/hillary_kipkemoi/hugging_face_models/garri/llama-2-amharic-3784m does not appear to have a file named config.json. Checkout 'https://huggingface.co//home/hillary_kipkemoi/hugging_face_models/garri/llama-2-amharic-3784m/tree/main' for available files."
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "checkpoint = \"iocuydi/llama-2-amharic-3784m\"\n",
    "commit_hash = \"04fcac974701f1dab0b8e39af9d3ecfce07b3773\"\n",
    "\n",
    "cache_dir = \"cache_dir\"\n",
    "# The commit hash is needed, because the model repo was rearranged after this commit (files -> finetuned/files),\n",
    "# and I couldn't load the model from the new structure\n",
    "\n",
    "# Load the pre-trained model\n",
    "llama_model = LlamaForCausalLM.from_pretrained(\n",
    "  \"/home/hillary_kipkemoi/hugging_face_models/garri/llama-2-amharic-3784m\",\n",
    ")\n",
    "\n",
    "# Resize the model's token embeddings\n",
    "llama_model.resize_token_embeddings(len(sp))\n",
    "\n",
    "# Create a data loader for your tokenized dataset\n",
    "# Here, 'input_ids' is assumed to be the key in your tokenized dataset for the tokenized texts\n",
    "data_loader = DataLoader(tokenized_dataset['train']['input_ids'], batch_size=32)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(llama_model.parameters())\n",
    "\n",
    "# Define the training loop\n",
    "for epoch in range(10):  # Number of epochs\n",
    "  for batch in data_loader:\n",
    "    # Move the batch tensors to the same device as the model\n",
    "    batch = batch.to(llama_model.device)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = llama_model(input_ids=batch)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = outputs.loss\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the model's parameters\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  print(f'Epoch {epoch+1} completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./results\",\n",
    "  num_train_epochs=3,\n",
    "  per_device_train_batch_size=16,\n",
    "  per_device_eval_batch_size=64,\n",
    "  warmup_steps=500,\n",
    "  weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Create the Trainer and train\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=tokenized_dataset[\"train\"],\n",
    "  eval_dataset=tokenized_dataset[\"test\"],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
