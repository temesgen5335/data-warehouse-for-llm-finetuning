{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from src.database import MongoDB\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'clean_data'\n",
    "collection_name = 'alain_news_clean'\n",
    "connection_string = 'mongodb://localhost:27017/'\n",
    "clean_db = MongoDB(db_name=db_name, collection_name=collection_name, connection_string=connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_db.remove_duplicates('article_url', 'alain_news_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all documents\n",
    "documents = list(clean_db.collection.find({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split documents into training and test sets\n",
    "train_docs, test_docs = train_test_split(documents, test_size=0.2)  # adjust the test_size as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(docs, filename):\n",
    "  with open(filename, 'w', encoding='utf8') as f:\n",
    "    for doc in docs:\n",
    "      f.write(doc['title'] + '\\n')\n",
    "      f.write(doc['summary'] + '\\n')\n",
    "      f.write(doc['content'] + '\\n')\n",
    "\n",
    "write_to_file(train_docs, 'amharic_train.txt')\n",
    "write_to_file(test_docs, 'amharic_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=amharic_train.txt --model_prefix=mine --vocab_size=2000\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: amharic_train.txt\n",
      "  input_format: \n",
      "  model_prefix: mine\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: amharic_train.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 97038 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=8012836\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9543% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=193\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999543\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 96956 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=3990246\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 160646 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 96956\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 128998\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 128998 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=77586 obj=12.1114 num_tokens=244098 num_tokens/piece=3.14616\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=66219 obj=10.5887 num_tokens=246366 num_tokens/piece=3.72047\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=49660 obj=10.5727 num_tokens=262805 num_tokens/piece=5.29209\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=49619 obj=10.5442 num_tokens=263089 num_tokens/piece=5.30218\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=37214 obj=10.6608 num_tokens=286932 num_tokens/piece=7.71032\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=37213 obj=10.6254 num_tokens=286953 num_tokens/piece=7.7111\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=27908 obj=10.8027 num_tokens=312247 num_tokens/piece=11.1884\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=27908 obj=10.759 num_tokens=312256 num_tokens/piece=11.1888\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=20931 obj=10.9854 num_tokens=337551 num_tokens/piece=16.1268\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=20931 obj=10.9327 num_tokens=337543 num_tokens/piece=16.1265\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=15698 obj=11.213 num_tokens=362043 num_tokens/piece=23.063\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=15698 obj=11.1505 num_tokens=362055 num_tokens/piece=23.0638\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=11773 obj=11.4832 num_tokens=386651 num_tokens/piece=32.8422\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=11773 obj=11.4107 num_tokens=386665 num_tokens/piece=32.8434\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8829 obj=11.7986 num_tokens=410550 num_tokens/piece=46.5002\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8829 obj=11.717 num_tokens=410557 num_tokens/piece=46.501\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6621 obj=12.1551 num_tokens=432495 num_tokens/piece=65.3217\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6621 obj=12.0654 num_tokens=432503 num_tokens/piece=65.3229\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4965 obj=12.5515 num_tokens=457804 num_tokens/piece=92.2062\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4965 obj=12.451 num_tokens=457805 num_tokens/piece=92.2064\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3723 obj=12.9796 num_tokens=481486 num_tokens/piece=129.327\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3723 obj=12.8695 num_tokens=481489 num_tokens/piece=129.328\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2792 obj=13.4543 num_tokens=504897 num_tokens/piece=180.837\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2792 obj=13.3404 num_tokens=504896 num_tokens/piece=180.837\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2200 obj=13.8523 num_tokens=522068 num_tokens/piece=237.304\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2200 obj=13.7534 num_tokens=522074 num_tokens/piece=237.306\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: mine.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: mine.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# My Amharic corpus is in 'amharic_corpus.txt'\n",
    "spm.SentencePieceTrainer.train('--input=amharic_train.txt --model_prefix=mine --vocab_size=2000')\n",
    "\n",
    "# 'm.model' and 'm.vocab' files will be created after training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the model and use it to tokenize new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁በ', '▁በ', 'አለማቀፍ', '▁ደረጃ', '▁የተ', 'ፈፀመ', 'ው', '▁የ', 'ሞ', 'ት', '▁ቅጣት']\n",
      "[8, 8, 1657, 324, 44, 504, 9, 5, 89, 7, 1466]\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('mine.model')  # replace 'm.model' with the path to your model\n",
    "\n",
    "# Tokenize Amharic text\n",
    "text = \"በ በአለማቀፍ ደረጃ የተፈፀመው የሞት ቅጣት\"\n",
    "\n",
    "# replace with your Amharic text\n",
    "tokens = sp.encode_as_pieces(text)\n",
    "ids = sp.encode_as_ids(text)\n",
    "print(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 8, 1657, 324, 44, 504, 9, 5, 89, 7, 1466]\n"
     ]
    }
   ],
   "source": [
    "# Encode the text\n",
    "encoded_text = sp.encode(text, out_type=int)\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the pre-trained model that has been trained on Amharic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'encoded_text' is your tokenized text\n",
    "inputs = tokenizer(tokens, return_tensors='pt', padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[101, 100, 102],\n",
       "        [101, 100, 102],\n",
       "        [101, 100, 102],\n",
       "        [101, 100, 102],\n",
       "        [101, 100, 102],\n",
       "        [101, 100, 102],\n",
       "        [101, 100, 102],\n",
       "        [101, 100, 102],\n",
       "        [101, 100, 102],\n",
       "        [101, 100, 102],\n",
       "        [101, 100, 102]]), 'token_type_ids': tensor([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
