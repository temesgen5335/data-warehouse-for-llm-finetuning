{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/10AcademyG4/data-warehouse-for-llm-finetuning/blob/hugging_face/sentencepiece_python_module_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOKaXwohvyDi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s9_7-b6lvuC_"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9BDzLVkUFT4"
      },
      "source": [
        "# Sentencepiece python module\n",
        "\n",
        "\n",
        "This notebook describes comprehensive examples of sentencepiece Python module.\n",
        "Since Python module calls C++ API through SWIG,  this document is also useful for developing C++ client."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIgXb6P2Yg6g"
      },
      "source": [
        "## Install and data preparation\n",
        "\n",
        "We use the small training data (botchan.txt) in this example.\n",
        "([Botchan](https://en.wikipedia.org/wiki/Botchan) is a novel written by Natsume Sōseki in 1906.  The sample is English-translated one.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k5KbVgiYae-"
      },
      "source": [
        "## Basic  end-to-end example\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SUcAbKnRVAv6"
      },
      "outputs": [],
      "source": [
        "# !pip install sentencepiece\n",
        "# !wget https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "ee9W6wGnVteW",
        "outputId": "c8cbe6d9-d052-4e6f-b5ab-270445a84f93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: /home/hilla/code/10Academy-training/week4/llm_datawarehouse/notebooks/tokenization\n",
            "['▁በ', 'ኬንያ', '▁በ', 'ኬንያ', '▁የ', 'ም', 'ር', 'ጫ', '▁', 'ታ', 'ሪ', 'ክ', '▁ለመ', 'ጀ', 'መ', 'ሪ', 'ያ', '▁', 'ጊ', 'ዜ', '▁የ', 'ህ', 'ግ', '▁', 'ታ', 'ራ', 'ሚ', 'ዎ', 'ች', '▁', 'ድ', 'ም', 'ጽ', '▁ሰ', 'ጡ', '፡፡']\n",
            "[7, 126, 7, 126, 16, 6, 8, 226, 3, 17, 19, 56, 149, 128, 59, 19, 21, 3, 203, 196, 16, 73, 166, 3, 17, 13, 176, 194, 43, 3, 42, 6, 0, 113, 131, 0]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=amharic.txt --model_prefix=m --vocab_size=238\n",
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: amharic.txt\n",
            "  input_format: \n",
            "  model_prefix: m\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 238\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(185) LOG(INFO) Loading corpus: amharic.txt\n",
            "trainer_interface.cc(409) LOG(INFO) Loaded all 18 sentences\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(539) LOG(INFO) all chars count=1659\n",
            "trainer_interface.cc(560) LOG(INFO) Alphabet size=126\n",
            "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
            "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 18 sentences.\n",
            "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=719\n",
            "unigram_model_trainer.cc(312) LOG(INFO) Initialized 324 seed sentencepieces\n",
            "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 18\n",
            "trainer_interface.cc(609) LOG(INFO) Done! 217\n",
            "unigram_model_trainer.cc(602) LOG(INFO) Using 217 sentences for EM training\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=225 obj=16.523 num_tokens=709 num_tokens/piece=3.15111\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=221 obj=15.3631 num_tokens=713 num_tokens/piece=3.22624\n",
            "trainer_interface.cc(687) LOG(INFO) Saving model: m.model\n",
            "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab\n"
          ]
        }
      ],
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# train sentencepiece model from `amharic.txt` and makes `m.model` and `m.vocab`\n",
        "# `m.vocab` is just a reference. not used in the segmentation.\n",
        "# Print the current working directory\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "\n",
        "# Check if the file exists in the current working directory\n",
        "if os.path.exists('amharic.txt'):\n",
        "    spm.SentencePieceTrainer.train('--input=amharic.txt --model_prefix=m --vocab_size=238')\n",
        "else:\n",
        "    print(\"The file 'amharic.txt' does not exist in the current working directory.\")\n",
        "\n",
        "# makes segmenter instance and loads the model file (m.model)\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "# English: \"In Kenya, for the first time, prisoners voted in the election.\"\n",
        "\n",
        "# encode: text => id\n",
        "print(sp.encode_as_pieces('በኬንያ በኬንያ የምርጫ ታሪክ ለመጀመሪያ ጊዜ የህግ ታራሚዎች ድምጽ ሰጡ፡፡'))\n",
        "print(sp.encode_as_ids('በኬንያ  በኬንያ የምርጫ ታሪክ ለመጀመሪያ ጊዜ የህግ ታራሚዎች ድምጽ ሰጡ፡፡'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Explanation of the output above:\n",
        "The output you're seeing is related to the training of a SentencePiece model on your `amharic.txt` file.\n",
        "\n",
        "1. `Current working directory: /home/hilla/code/10Academy-training/week4/llm_datawarehouse/notebooks/tokenization`: This is the directory where your Python script is running.\n",
        "\n",
        "2. `['▁በ', 'ኬንያ', '▁በ', 'ኬንያ', '▁የ', 'ም', 'ር', 'ጫ', '▁', 'ታ', 'ሪ', 'ክ', '▁ለመ', 'ጀ', 'መ', 'ሪ', 'ያ', '▁', 'ጊ', 'ዜ', '▁የ', 'ህ', 'ግ', '▁', 'ታ', 'ራ', 'ሚ', 'ዎ', 'ች', '▁', 'ድ', 'ም', 'ጽ', '▁ሰ', 'ጡ', '፡፡']`: This is a list of tokens generated by the SentencePiece model from a sentence in your `amharic.txt` file.\n",
        "\n",
        "3. `[7, 126, 7, 126, 16, 6, 8, 226, 3, 17, 19, 56, 149, 128, 59, 19, 21, 3, 203, 196, 16, 73, 166, 3, 17, 13, 176, 194, 43, 3, 42, 6, 0, 113, 131, 0]`: This is a list of the corresponding IDs for each token in the previous list.\n",
        "\n",
        "4. The rest of the output is logging information from the SentencePiece model training process. It includes details about the training configuration (e.g., `vocab_size: 238`, `model_type: UNIGRAM`, `character_coverage: 0.9995`, etc.) and the progress of the training (e.g., `EM sub_iter=0 size=225 obj=16.523 num_tokens=709 num_tokens/piece=3.15111`).\n",
        "\n",
        "5. `trainer_interface.cc(687) LOG(INFO) Saving model: m.model` and `trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab`: These lines indicate that the trained model and vocabulary are being saved to the files `m.model` and `m.vocab`, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "the amharic.txt file was used to train the SentencePiece model and generate the m.model and m.vocab files. The model learned to tokenize text and assign IDs to tokens based on the text in the amharic.txt file.\n",
        "\n",
        "If you want to tokenize a new text and get the corresponding IDs, you should use the same SentencePiece model. The new text doesn't have to be in the amharic.txt file, but it should be in the same language and use the same script (in this case, Amharic and the Ge'ez script)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IDs: [109, 16, 223, 8, 163, 190, 183, 7, 28, 38, 14, 8, 6, 201, 114, 60, 30, 42, 46, 37, 26, 6, 0, 20]\n"
          ]
        }
      ],
      "source": [
        "def file_to_ids(filename):\n",
        "  # Read the text from the file\n",
        "  with open(filename, 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "  # Tokenize the text and get the IDs of the tokens\n",
        "  ids = sp.encode_as_ids(text)\n",
        "  return ids\n",
        "\n",
        "# Use the function to get the IDs of a text in a file\n",
        "ids = file_to_ids('new_amharic.txt')\n",
        "print(\"IDs:\", ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FDat-_kWvuDG",
        "outputId": "afa3ec26-e042-4563-802f-fcef4dcf1666"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "በኬንያ▁የምርጫ▁ታሪክ ለመጀመሪያ▁ጊዜ▁የህግ▁ታራሚዎች▁ድምጽ ሰጡ፡፡\n",
            "የእስራኤል የጦር ካቢኔ በኢራን ላይ እርምጃ እንዲወሰድ ተስማም ⁇ ል\n"
          ]
        }
      ],
      "source": [
        "# decode: id => text\n",
        "pieces = ['▁በ', 'ኬንያ', '▁የምርጫ', '▁ታሪክ', '▁ለ', 'መጀመሪያ', '▁ጊዜ', '▁የህግ', '▁ታ', 'ራ', 'ሚ', 'ዎች', '▁ድምጽ', '▁', 'ሰጡ', '፡፡']\n",
        "print(sp.decode_pieces(pieces))\n",
        "print(sp.decode_ids(ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output Explanation:\n",
        "\n",
        "*sp.decode_pieces(pieces)*: This function takes a list of tokens as input and returns a string where each token has been replaced with its corresponding text. The tokens are pieces of text that the SentencePiece model has learned to recognize during training. The underscore character ('▁') at the beginning of some tokens represents a space in the original text.\n",
        "\n",
        "*p.decode_ids(ids)*: This function takes a list of IDs as input and returns a string where each ID has been replaced with its corresponding text. The IDs are unique identifiers that the SentencePiece model has assigned to each unique token in its vocabulary.\n",
        "\n",
        "##### Encoding and Decoding\n",
        "The process of encoding and decoding is a fundamental part of working with natural language processing (NLP) models.\n",
        "\n",
        "Encoding: This is the process of converting raw text into a format (like tokens or IDs) that can be understood by a machine learning model. In this case, the SentencePiece model is used to break down the text into smaller pieces (tokens) and assign each unique token a unique ID. This is necessary because machine learning models don't understand raw text, they work with numerical data.\n",
        "\n",
        "Decoding: This is the reverse process of encoding. It converts the model's output from the numerical format back into human-readable text. In this case, the SentencePiece model is used to convert a list of tokens or IDs back into text. This is necessary to interpret the model's output in a meaningful way.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "4vHnQbBOltZo",
        "outputId": "9bb1ecaf-2883-494c-e34b-5616efac126a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "238\n",
            "▁በ\n",
            "7\n",
            "0\n",
            "126\n",
            "<unk> False\n",
            "<s> True\n",
            "</s> True\n"
          ]
        }
      ],
      "source": [
        "# returns vocab size, which is the total number of unique tokens that the model has learned to recognize.\n",
        "print(sp.get_piece_size())\n",
        "\n",
        "# id <=> piece conversion\n",
        "\n",
        "# takes an ID as input and returns the corresponding token.\n",
        "print(sp.id_to_piece(7))\n",
        "\n",
        "# takes a token as input and returns the corresponding ID.\n",
        "print(sp.piece_to_id('▁በ'))\n",
        "\n",
        "# returns 0 for unknown tokens (we can change the id for UNK)\n",
        "'''\n",
        "This function below tries to find the ID that corresponds to the token 'MUST_BE_UNKNOWN'. \n",
        "If the token is not in the model's vocabulary, the function returns 0, which is the ID for unknown tokens (<unk>).\n",
        "'''\n",
        "print(sp.piece_to_id('__MUST_BE_UNKNOWN__'))\n",
        "\n",
        "# returns 126 for 'ኬንያ', we can use this to check if a token is in the vocabulary, we defined this before\n",
        "print(sp.piece_to_id('ኬንያ'))\n",
        "\n",
        "# <unk>, <s>, </s> are defined by default. Their ids are (0, 1, 2)\n",
        "# <s> and </s> are defined as 'control' symbol.\n",
        "\n",
        "'''\n",
        " For each ID, the loop prints the corresponding token and a boolean value that indicates whether the token is a control symbol. \n",
        " Control symbols are special tokens that are used to control the behavior of the model, but they don't represent any actual text.\n",
        "'''\n",
        "for id in range(3):\n",
        "  print(sp.id_to_piece(id), sp.is_control(id))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRv6EeC2Y2PE"
      },
      "source": [
        "## Loads model from byte stream\n",
        "\n",
        "Sentencepiece's model file is just a serialized [protocol buffer](https://developers.google.com/protocol-buffers/). We can instantiate sentencepiece processor from byte object with **load_from_serialized_proto** method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQat81LNvuDJ"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "I7kavBOHvuDJ",
        "outputId": "f961b5b0-637f-44fd-d431-da07660a48d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bos= 1\n",
            "eos= 2\n",
            "unk= 0\n",
            "pad= -1\n",
            "[7, 126, 16, 6, 8, 226, 3, 17, 19, 56, 149, 128, 59, 19, 21, 3, 203, 196, 16, 73, 166, 3, 17, 13, 176, 194, 43, 3, 42, 6, 0, 113, 131, 0]\n",
            "[1, 7, 126, 16, 6, 8, 226, 3, 17, 19, 56, 149, 128, 59, 19, 21, 3, 203, 196, 16, 73, 166, 3, 17, 13, 176, 194, 43, 3, 42, 6, 0, 113, 131, 0, 2]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=amharic.txt --model_prefix=m --vocab_size=238\n",
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: amharic.txt\n",
            "  input_format: \n",
            "  model_prefix: m\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 238\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(185) LOG(INFO) Loading corpus: amharic.txt\n",
            "trainer_interface.cc(409) LOG(INFO) Loaded all 18 sentences\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(539) LOG(INFO) all chars count=1659\n",
            "trainer_interface.cc(560) LOG(INFO) Alphabet size=126\n",
            "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
            "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 18 sentences.\n",
            "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=719\n",
            "unigram_model_trainer.cc(312) LOG(INFO) Initialized 324 seed sentencepieces\n",
            "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 18\n",
            "trainer_interface.cc(609) LOG(INFO) Done! 217\n",
            "unigram_model_trainer.cc(602) LOG(INFO) Using 217 sentences for EM training\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=225 obj=16.523 num_tokens=709 num_tokens/piece=3.15111\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=221 obj=15.3631 num_tokens=713 num_tokens/piece=3.22624\n",
            "trainer_interface.cc(687) LOG(INFO) Saving model: m.model\n",
            "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab\n"
          ]
        }
      ],
      "source": [
        "spm.SentencePieceTrainer.train('--input=amharic.txt --model_prefix=m --vocab_size=238')\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "print('bos=', sp.bos_id())\n",
        "print('eos=', sp.eos_id())\n",
        "print('unk=', sp.unk_id())\n",
        "print('pad=', sp.pad_id())  # disabled by default\n",
        "\n",
        "\n",
        "print(sp.encode_as_ids('በኬንያ የምርጫ ታሪክ ለመጀመሪያ ጊዜ የህግ ታራሚዎች ድምጽ ሰጡ፡፡'))\n",
        "\n",
        "# Prepend or append bos/eos ids.\n",
        "print([sp.bos_id()] + sp.encode_as_ids('በኬንያ የምርጫ ታሪክ ለመጀመሪያ ጊዜ የህግ ታራሚዎች ድምጽ ሰጡ፡፡') + [sp.eos_id()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0Bdi9SuxYAud",
        "outputId": "b1566541-288e-4aa3-9c75-e494d0ab276a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-05-30 09:12:53.300954: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-05-30 09:12:53.309954: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-05-30 09:12:53.373441: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-05-30 09:12:55.183474: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁በ', 'ኬንያ', '▁የ', 'ም', 'ር', 'ጫ', '▁', 'ታ', 'ሪ', 'ክ', '▁ለመ', 'ጀ', 'መ', 'ሪ', 'ያ', '▁', 'ጊ', 'ዜ', '▁የ', 'ህ', 'ግ', '▁', 'ታ', 'ራ', 'ሚ', 'ዎ', 'ች', '▁', 'ድ', 'ም', 'ጽ', '▁ሰ', 'ጡ', '፡፡']\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Assumes that m.model is stored in non-Posix file system.\n",
        "serialized_model_proto = tf.io.gfile.GFile('m.model', 'rb').read()\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load_from_serialized_proto(serialized_model_proto)\n",
        "\n",
        "print(sp.encode_as_pieces('በኬንያ የምርጫ ታሪክ ለመጀመሪያ ጊዜ የህግ ታራሚዎች ድምጽ ሰጡ፡፡ '))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Explanation of the code and output above:\n",
        "\n",
        "This code is using the SentencePiece library to load a pre-trained subword tokenization model and then use that model to tokenize some text.\n",
        "\n",
        "Here's a breakdown of what each part of the code does:\n",
        "\n",
        "1. `import tensorflow as tf`: This line imports the TensorFlow library. TensorFlow is a machine learning framework, but in this code, it's being used for its file I/O capabilities.\n",
        "\n",
        "2. `serialized_model_proto = tf.io.gfile.GFile('m.model', 'rb').read()`: This line reads the contents of the file `m.model` into a byte string. The `m.model` file is assumed to contain a SentencePiece model that has been serialized (i.e., converted into a format that can be stored or transmitted) using Protocol Buffers, which is a language-neutral, platform-neutral, extensible mechanism for serializing structured data.\n",
        "\n",
        "3. `sp = spm.SentencePieceProcessor()`: This line creates a new `SentencePieceProcessor` object, which is used to load the trained model and tokenize text.\n",
        "\n",
        "4. `sp.load_from_serialized_proto(serialized_model_proto)`: This line loads the trained model from the serialized Protocol Buffers string.\n",
        "\n",
        "5. `print(sp.encode_as_pieces('በኬንያ የምርጫ ታሪክ ለመጀመሪያ ጊዜ የህግ ታራሚዎች ድምጽ ሰጡ፡፡ '))`: This line tokenizes the given Amharic text into a list of subwords (or \"pieces\") and prints the list.\n",
        "\n",
        "The output of the code is the list of subwords that the SentencePiece model has split the text into. This is a common step in preparing text for use in natural language processing tasks, as it allows models to handle words that they haven't seen before by breaking them down into known subwords."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imfPyYlVZmxz"
      },
      "source": [
        "## User defined and control symbols\n",
        "\n",
        "We can define special tokens (symbols) to tweak the DNN behavior through the tokens.   Typical examples are  [BERT](https://arxiv.org/abs/1810.04805)'s special symbols., e.g., [SEP] and [CLS].\n",
        "\n",
        "There are two types of special tokens:\n",
        "\n",
        "- **user defined symbols**: Always treated as one token in any context. These symbols can appear in the input sentence.\n",
        "- **control symbol**:  We only reserve ids for these tokens. Even if these tokens appear in the input text, they are not handled as one token. User needs to insert ids explicitly after encoding.\n",
        "\n",
        "For experimental purposes, user defined symbols are easier to use since user can change the behavior just by modifying the input text. However,  we want to use control symbols in the production setting in order to avoid users from tweaking the behavior by feeding these special symbols in their input text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "dngckiPMcWbA",
        "outputId": "e52883b1-6452-4a90-8802-945c9ac9d5b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁', 'this', '▁', 'is', '▁', 'a', '▁', 'test', '<sep>', '▁', 'hello', '▁', 'world', '<cls>']\n",
            "3\n",
            "4\n",
            "3= <sep>\n",
            "4= <cls>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=amharic.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls>,<start> --vocab_size=238\n",
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: amharic.txt\n",
            "  input_format: \n",
            "  model_prefix: m_user\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 238\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  user_defined_symbols: <sep>\n",
            "  user_defined_symbols: <cls>\n",
            "  user_defined_symbols: <start>\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(185) LOG(INFO) Loading corpus: amharic.txt\n",
            "trainer_interface.cc(409) LOG(INFO) Loaded all 18 sentences\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <sep>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <cls>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <start>\n",
            "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(539) LOG(INFO) all chars count=1659\n",
            "trainer_interface.cc(560) LOG(INFO) Alphabet size=126\n",
            "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
            "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 18 sentences.\n",
            "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=719\n",
            "unigram_model_trainer.cc(312) LOG(INFO) Initialized 324 seed sentencepieces\n",
            "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 18\n",
            "trainer_interface.cc(609) LOG(INFO) Done! 217\n",
            "unigram_model_trainer.cc(602) LOG(INFO) Using 217 sentences for EM training\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=225 obj=16.523 num_tokens=709 num_tokens/piece=3.15111\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=221 obj=15.3631 num_tokens=713 num_tokens/piece=3.22624\n",
            "trainer_interface.cc(687) LOG(INFO) Saving model: m_user.model\n",
            "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m_user.vocab\n"
          ]
        }
      ],
      "source": [
        "# Example of user defined symbols\n",
        "spm.SentencePieceTrainer.train('--input=amharic.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls>,<start> --vocab_size=238')\n",
        "\n",
        "sp_user = spm.SentencePieceProcessor()\n",
        "sp_user.load('m_user.model')\n",
        "\n",
        "# ids are reserved in both mode.\n",
        "# <unk>=0, <s>=1, </s>=2, <sep>=3, <cls>=4\n",
        "# user defined symbols allow these symbols to appear in the text.\n",
        "print(sp_user.encode_as_pieces('this is a test<sep> hello world<cls>'))\n",
        "print(sp_user.piece_to_id('<sep>'))  # 3\n",
        "print(sp_user.piece_to_id('<cls>'))  # 4\n",
        "print('3=', sp_user.decode_ids([3]))  # decoded to <sep>\n",
        "print('4=', sp_user.decode_ids([4]))  # decoded to <cls>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Explanation of the code and output above:\n",
        "This code is using the SentencePiece library to train a new subword tokenization model with user-defined symbols, and then use that model to tokenize some text.\n",
        "\n",
        "Here's a breakdown of what each part of the code does:\n",
        "\n",
        "1. `spm.SentencePieceTrainer.train('--input=amharic.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls>,<start> --vocab_size=238')`: This line trains a new SentencePiece model on the text in `amharic.txt`. The model is saved to a file named `m_user.model`, and its vocabulary will contain 238 unique tokens. The `--user_defined_symbols=<sep>,<cls>,<start>` option tells SentencePiece to treat `<sep>`, `<cls>`, and `<start>` as user-defined symbols. These symbols will be treated as single tokens during tokenization, even if they are not surrounded by whitespace.\n",
        "\n",
        "2. `sp_user = spm.SentencePieceProcessor()`: This line creates a new `SentencePieceProcessor` object, which is used to load the trained model and tokenize text.\n",
        "\n",
        "3. `sp_user.load('m_user.model')`: This line loads the trained model from `m_user.model`.\n",
        "\n",
        "4. `print(sp_user.encode_as_pieces('this is a test<sep> hello world<cls>'))`: This line tokenizes the given text into a list of subwords (or \"pieces\") and prints the list. The `<sep>` and `<cls>` user-defined symbols are treated as separate tokens.\n",
        "\n",
        "5. The next three `print` statements print the ID of the `<sep>` and `<cls>` tokens and the tokens that correspond to the IDs 3 and 4. Unlike control symbols, user-defined symbols are not decoded to empty strings, so `sp_user.decode_ids([3])` and `sp_user.decode_ids([4])` return `<sep>` and `<cls>`, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- This code is similar to the previous one, but it uses control symbols instead of user-defined symbols. Control symbols and user-defined symbols are two different ways to handle special tokens in SentencePiece.\n",
        "\n",
        "In the ouput, the `<sep>` and `<cls>` control symbols are treated as part of the preceding words (`test<sep>` and `world<cls>`) because control symbols in SentencePiece are not split from their adjacent tokens unless they are separated by a space.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "5awRJ0y1oYm-",
        "outputId": "a5fa1ef9-ee5f-4f7d-b6bd-b611979b7350"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁', 'this', '▁', 'is', '▁', 'a', '▁', 'test<sep>', '▁', 'hello', '▁', 'world<cls>']\n",
            "3\n",
            "4\n",
            "3= \n",
            "4= \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=amharic.txt --model_prefix=m_ctrl --control_symbols=<sep>,<cls> --vocab_size=238\n",
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: amharic.txt\n",
            "  input_format: \n",
            "  model_prefix: m_ctrl\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 238\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  control_symbols: <sep>\n",
            "  control_symbols: <cls>\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(185) LOG(INFO) Loading corpus: amharic.txt\n",
            "trainer_interface.cc(409) LOG(INFO) Loaded all 18 sentences\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <sep>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <cls>\n",
            "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(539) LOG(INFO) all chars count=1659\n",
            "trainer_interface.cc(560) LOG(INFO) Alphabet size=126\n",
            "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
            "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 18 sentences.\n",
            "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=719\n",
            "unigram_model_trainer.cc(312) LOG(INFO) Initialized 324 seed sentencepieces\n",
            "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 18\n",
            "trainer_interface.cc(609) LOG(INFO) Done! 217\n",
            "unigram_model_trainer.cc(602) LOG(INFO) Using 217 sentences for EM training\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=225 obj=16.523 num_tokens=709 num_tokens/piece=3.15111\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=221 obj=15.3631 num_tokens=713 num_tokens/piece=3.22624\n",
            "trainer_interface.cc(687) LOG(INFO) Saving model: m_ctrl.model\n",
            "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m_ctrl.vocab\n"
          ]
        }
      ],
      "source": [
        "# Example of control symbols\n",
        "spm.SentencePieceTrainer.train('--input=amharic.txt --model_prefix=m_ctrl --control_symbols=<sep>,<cls> --vocab_size=238')\n",
        "\n",
        "sp_ctrl = spm.SentencePieceProcessor()\n",
        "sp_ctrl.load('m_ctrl.model')\n",
        "\n",
        "# control symbols just reserve ids.\n",
        "print(sp_ctrl.encode_as_pieces('this is a test<sep> hello world<cls>'))\n",
        "print(sp_ctrl.piece_to_id('<sep>'))  # 3\n",
        "print(sp_ctrl.piece_to_id('<cls>'))  # 4\n",
        "print('3=', sp_ctrl.decode_ids([3]))  # decoded to empty\n",
        "print('4=', sp_ctrl.decode_ids([4]))  # decoded to empty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ppZck91s0rq"
      },
      "source": [
        " BOS/EOS (&lt;s&gt;, &lt;/s&gt;) are defined as control symbols, but we can define them as user defined symbols."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "PQoZ8paVhcEL",
        "outputId": "c17d2ae2-17cd-4875-997c-fe66dba23dde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁', '<', 's', '>', '▁he', 'll', 'o', '</', 's', '>']\n",
            "['▁', '<s>', '▁he', 'll', 'o', '</s>']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=botchan.txt --model_prefix=m_bos_as_user --user_defined_symbols=<s>,</s> --vocab_size=2000\n",
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: botchan.txt\n",
            "  input_format: \n",
            "  model_prefix: m_bos_as_user\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 2000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  user_defined_symbols: <s>\n",
            "  user_defined_symbols: </s>\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(185) LOG(INFO) Loading corpus: botchan.txt\n",
            "trainer_interface.cc(409) LOG(INFO) Loaded all 4288 sentences\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(539) LOG(INFO) all chars count=274252\n",
            "trainer_interface.cc(550) LOG(INFO) Done: 99.957% characters are covered.\n",
            "trainer_interface.cc(560) LOG(INFO) Alphabet size=69\n",
            "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.99957\n",
            "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 4288 sentences.\n",
            "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=144687\n",
            "unigram_model_trainer.cc(312) LOG(INFO) Initialized 16112 seed sentencepieces\n",
            "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 4288\n",
            "trainer_interface.cc(609) LOG(INFO) Done! 9165\n",
            "unigram_model_trainer.cc(602) LOG(INFO) Using 9165 sentences for EM training\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5926 obj=10.5283 num_tokens=18931 num_tokens/piece=3.19457\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5232 obj=8.64492 num_tokens=19009 num_tokens/piece=3.63322\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3923 obj=8.71845 num_tokens=20449 num_tokens/piece=5.21259\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3921 obj=8.66278 num_tokens=20450 num_tokens/piece=5.21551\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2940 obj=8.96602 num_tokens=22797 num_tokens/piece=7.75408\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2940 obj=8.88624 num_tokens=22798 num_tokens/piece=7.75442\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2205 obj=9.2744 num_tokens=25530 num_tokens/piece=11.5782\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2205 obj=9.18615 num_tokens=25533 num_tokens/piece=11.5796\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2200 obj=9.18811 num_tokens=25552 num_tokens/piece=11.6145\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2200 obj=9.18747 num_tokens=25552 num_tokens/piece=11.6145\n",
            "trainer_interface.cc(687) LOG(INFO) Saving model: m_bos_as_user.model\n",
            "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m_bos_as_user.vocab\n"
          ]
        }
      ],
      "source": [
        "spm.SentencePieceTrainer.train('--input=amharic.txt --model_prefix=m_bos_as_user --user_defined_symbols=<s>,</s> --vocab_size=2000')\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "print(sp.encode_as_pieces('<s> hello</s>'))   # <s>,</s> are segmented. (default behavior)\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m_bos_as_user.model')\n",
        "print(sp.encode_as_pieces('<s> hello</s>'))   # <s>,</s> are handled as one token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ2GjO5Tmjk9"
      },
      "source": [
        "## Manipulating BOS/EOS/EOS/PAD symbols\n",
        "\n",
        "BOS, EOS, UNK, and PAD ids can be obtained with **bos_id()**, **eos_id()**,  **unk_id()**, and **pad_id()** methods. We can explicitly insert these ids as follows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6ezXouSvuDL"
      },
      "source": [
        "The BOS, EOS, UNK, and PAD token IDs that are being printed are not specific to the \"Hello world\" text, but rather are part of the SentencePiece model itself.\n",
        "\n",
        "When you train a SentencePiece model, it creates a vocabulary of tokens, which includes the special control tokens like BOS, EOS, UNK, and PAD. These special tokens are assigned specific IDs within the model's vocabulary, and their IDs can be retrieved using the methods shown in the code:\n",
        "\n",
        "* sp.bos_id(): Returns the ID of the Beginning of Sequence token.\n",
        "* sp.eos_id(): Returns the ID of the End of Sequence token.\n",
        "* sp.unk_id(): Returns the ID of the Unknown token.\n",
        "* sp.pad_id(): Returns the ID of the Padding token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "UtFQqK3tmp7G",
        "outputId": "03e82bec-be40-4574-ab62-c66cdc3f28a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bos= 1\n",
            "eos= 2\n",
            "unk= 0\n",
            "pad= -1\n",
            "[7, 798, 1055, 1152, 24, 1004, 166, 1492, 210, 19, 98, 57, 1767, 3, 532, 101]\n",
            "['▁በ', 'ኬንያ', '▁የምርጫ', '▁ታሪክ', '▁ለ', 'መጀመሪያ', '▁ጊዜ', '▁የህግ', '▁ታ', 'ራ', 'ሚ', 'ዎች', '▁ድምጽ', '▁', 'ሰጡ', '፡፡']\n",
            "[1, 7, 798, 1055, 1152, 24, 1004, 166, 1492, 210, 19, 98, 57, 1767, 3, 532, 101, 2]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=amharic.txt --model_prefix=m --vocab_size=2000\n",
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: amharic.txt\n",
            "  input_format: \n",
            "  model_prefix: m\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 2000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(185) LOG(INFO) Loading corpus: amharic.txt\n",
            "trainer_interface.cc(380) LOG(WARNING) Found too long line (5672 > 4192).\n",
            "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
            "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
            "trainer_interface.cc(409) LOG(INFO) Loaded all 39057 sentences\n",
            "trainer_interface.cc(416) LOG(INFO) Skipped 694 too long sentences.\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(539) LOG(INFO) all chars count=15040288\n",
            "trainer_interface.cc(550) LOG(INFO) Done: 99.9518% characters are covered.\n",
            "trainer_interface.cc(560) LOG(INFO) Alphabet size=237\n",
            "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999518\n",
            "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 38926 sentences.\n",
            "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=6788875\n",
            "unigram_model_trainer.cc(312) LOG(INFO) Initialized 377159 seed sentencepieces\n",
            "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 38926\n",
            "trainer_interface.cc(609) LOG(INFO) Done! 303267\n",
            "unigram_model_trainer.cc(602) LOG(INFO) Using 303267 sentences for EM training\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=148570 obj=12.6326 num_tokens=599116 num_tokens/piece=4.03255\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=129289 obj=11.0193 num_tokens=601387 num_tokens/piece=4.65149\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=96934 obj=11.0117 num_tokens=632315 num_tokens/piece=6.52315\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=96774 obj=10.986 num_tokens=632653 num_tokens/piece=6.53743\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=72579 obj=11.0866 num_tokens=674423 num_tokens/piece=9.29226\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=72571 obj=11.0592 num_tokens=674617 num_tokens/piece=9.29596\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=54427 obj=11.2001 num_tokens=719827 num_tokens/piece=13.2255\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=54426 obj=11.1668 num_tokens=719822 num_tokens/piece=13.2257\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=40819 obj=11.3491 num_tokens=766746 num_tokens/piece=18.784\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=40819 obj=11.3073 num_tokens=766725 num_tokens/piece=18.7835\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=30614 obj=11.5337 num_tokens=815166 num_tokens/piece=26.6272\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=30614 obj=11.4824 num_tokens=815184 num_tokens/piece=26.6278\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=22960 obj=11.7555 num_tokens=863615 num_tokens/piece=37.6139\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=22960 obj=11.6948 num_tokens=863636 num_tokens/piece=37.6148\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=17220 obj=12.0169 num_tokens=914727 num_tokens/piece=53.12\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=17220 obj=11.9466 num_tokens=914749 num_tokens/piece=53.1213\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12915 obj=12.3247 num_tokens=968463 num_tokens/piece=74.9875\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=12915 obj=12.2407 num_tokens=968487 num_tokens/piece=74.9893\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=9686 obj=12.6702 num_tokens=1024052 num_tokens/piece=105.725\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9686 obj=12.5759 num_tokens=1024266 num_tokens/piece=105.747\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=7264 obj=13.0632 num_tokens=1083459 num_tokens/piece=149.155\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=7264 obj=12.9563 num_tokens=1083497 num_tokens/piece=149.16\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5448 obj=13.4936 num_tokens=1144863 num_tokens/piece=210.144\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5448 obj=13.3756 num_tokens=1144902 num_tokens/piece=210.151\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4086 obj=13.979 num_tokens=1211177 num_tokens/piece=296.421\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4086 obj=13.8507 num_tokens=1211180 num_tokens/piece=296.422\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3064 obj=14.477 num_tokens=1276819 num_tokens/piece=416.716\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3064 obj=14.3407 num_tokens=1278019 num_tokens/piece=417.108\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2298 obj=15.0081 num_tokens=1339851 num_tokens/piece=583.051\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2298 obj=14.8617 num_tokens=1339841 num_tokens/piece=583.047\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2200 obj=14.9505 num_tokens=1351816 num_tokens/piece=614.462\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2200 obj=14.9298 num_tokens=1351817 num_tokens/piece=614.462\n",
            "trainer_interface.cc(687) LOG(INFO) Saving model: m.model\n",
            "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab\n"
          ]
        }
      ],
      "source": [
        "spm.SentencePieceTrainer.train('--input=amharic.txt --model_prefix=m --vocab_size=2000')\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "print('bos=', sp.bos_id())\n",
        "print('eos=', sp.eos_id())\n",
        "print('unk=', sp.unk_id())\n",
        "print('pad=', sp.pad_id())  # disabled by default\n",
        "\n",
        "# print(sp.encode_as_ids('በኬንያ የምርጫ ታሪክ ለመጀመሪያ ጊዜ የህግ ታራሚዎች ድምጽ ሰጡ፡፡'))\n",
        "# print(sp.encode_as_pieces('በኬንያ የምርጫ ታሪክ ለመጀመሪያ ጊዜ የህግ ታራሚዎች ድምጽ ሰጡ፡፡'))\n",
        "# # Prepend or append bos/eos ids.\n",
        "# print([sp.bos_id()] + sp.encode_as_ids('በኬንያ የምርጫ ታሪክ ለመጀመሪያ ጊዜ የህግ ታራሚዎች ድምጽ ሰጡ፡፡') + [sp.eos_id()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0Ehq_uYvuDM",
        "outputId": "dc974a14-6289-49ab-dee2-88366aa3e9dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁this', '▁is', '▁a', '▁t', 'est', '<sep>', '▁he', 'll', 'o', '▁world', '<cls>']\n",
            "3\n",
            "4\n",
            "3= <sep>\n",
            "4= <cls>\n"
          ]
        }
      ],
      "source": [
        "# Example of user defined symbols\n",
        "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls> --vocab_size=2000')\n",
        "\n",
        "sp_user = spm.SentencePieceProcessor()\n",
        "sp_user.load('m_user.model')\n",
        "\n",
        "# ids are reserved in both mode.\n",
        "# <unk>=0, <s>=1, </s>=2, <sep>=3, <cls>=4\n",
        "# user defined symbols allow these symbols to appear in the text.\n",
        "print(sp_user.encode_as_pieces('this is a test<sep> hello world<cls>'))\n",
        "print(sp_user.piece_to_id('<sep>'))  # 3\n",
        "print(sp_user.piece_to_id('<cls>'))  # 4\n",
        "print('3=', sp_user.decode_ids([3]))  # decoded to <sep>\n",
        "print('4=', sp_user.decode_ids([4]))  # decoded to <cls>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CLaMlHUh4Dk"
      },
      "source": [
        "## Changing the vocab id and surface representation of UNK/BOS/EOS/PAD symbols\n",
        "\n",
        "By default, UNK/BOS/EOS/PAD tokens and their ids are defined as follows:\n",
        "\n",
        "|token|UNK|BOS|EOS|PAD|\n",
        "---|---\n",
        "|surface|&lt;unk&gt;|&lt;s&gt;|&lt;/s&gt;|&lt;pad&gt;|\n",
        "|id|0|1|2|undefined (-1)|\n",
        "\n",
        "\n",
        "We can change these mappings with **--{unk|bos|eos|pad}_id** and **--{unk|bos|eos|pad}_piece** flags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "PKn1f3eih_We",
        "outputId": "a9349dd9-0cd7-49d7-8ed3-5b35658b3116"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PAD] True\n",
            "[UNK] False\n",
            "[BOS] True\n",
            "[EOS] True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=amharic.txt --vocab_size=2000 --model_prefix=m --pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3 --pad_piece=[PAD] --unk_piece=[UNK] --bos_piece=[BOS] --eos_piece=[EOS]\n",
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: amharic.txt\n",
            "  input_format: \n",
            "  model_prefix: m\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 2000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 1\n",
            "  bos_id: 2\n",
            "  eos_id: 3\n",
            "  pad_id: 0\n",
            "  unk_piece: [UNK]\n",
            "  bos_piece: [BOS]\n",
            "  eos_piece: [EOS]\n",
            "  pad_piece: [PAD]\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(185) LOG(INFO) Loading corpus: amharic.txt\n",
            "trainer_interface.cc(380) LOG(WARNING) Found too long line (5672 > 4192).\n",
            "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
            "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
            "trainer_interface.cc(409) LOG(INFO) Loaded all 39057 sentences\n",
            "trainer_interface.cc(416) LOG(INFO) Skipped 694 too long sentences.\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: [PAD]\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: [UNK]\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: [BOS]\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: [EOS]\n",
            "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(539) LOG(INFO) all chars count=15040288\n",
            "trainer_interface.cc(550) LOG(INFO) Done: 99.9518% characters are covered.\n",
            "trainer_interface.cc(560) LOG(INFO) Alphabet size=237\n",
            "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999518\n",
            "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 38926 sentences.\n",
            "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=6788875\n",
            "unigram_model_trainer.cc(312) LOG(INFO) Initialized 377159 seed sentencepieces\n",
            "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 38926\n",
            "trainer_interface.cc(609) LOG(INFO) Done! 303267\n",
            "unigram_model_trainer.cc(602) LOG(INFO) Using 303267 sentences for EM training\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=148570 obj=12.6326 num_tokens=599116 num_tokens/piece=4.03255\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=129289 obj=11.0193 num_tokens=601387 num_tokens/piece=4.65149\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=96934 obj=11.0117 num_tokens=632315 num_tokens/piece=6.52315\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=96774 obj=10.986 num_tokens=632653 num_tokens/piece=6.53743\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=72579 obj=11.0866 num_tokens=674423 num_tokens/piece=9.29226\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=72571 obj=11.0592 num_tokens=674617 num_tokens/piece=9.29596\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=54427 obj=11.2001 num_tokens=719827 num_tokens/piece=13.2255\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=54426 obj=11.1668 num_tokens=719822 num_tokens/piece=13.2257\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=40819 obj=11.3491 num_tokens=766746 num_tokens/piece=18.784\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=40819 obj=11.3073 num_tokens=766725 num_tokens/piece=18.7835\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=30614 obj=11.5337 num_tokens=815166 num_tokens/piece=26.6272\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=30614 obj=11.4824 num_tokens=815184 num_tokens/piece=26.6278\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=22960 obj=11.7555 num_tokens=863615 num_tokens/piece=37.6139\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=22960 obj=11.6948 num_tokens=863636 num_tokens/piece=37.6148\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=17220 obj=12.0169 num_tokens=914727 num_tokens/piece=53.12\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=17220 obj=11.9466 num_tokens=914749 num_tokens/piece=53.1213\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12915 obj=12.3247 num_tokens=968463 num_tokens/piece=74.9875\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=12915 obj=12.2407 num_tokens=968487 num_tokens/piece=74.9893\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=9686 obj=12.6702 num_tokens=1024052 num_tokens/piece=105.725\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9686 obj=12.5759 num_tokens=1024266 num_tokens/piece=105.747\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=7264 obj=13.0632 num_tokens=1083459 num_tokens/piece=149.155\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=7264 obj=12.9563 num_tokens=1083497 num_tokens/piece=149.16\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5448 obj=13.4936 num_tokens=1144863 num_tokens/piece=210.144\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5448 obj=13.3756 num_tokens=1144902 num_tokens/piece=210.151\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4086 obj=13.979 num_tokens=1211177 num_tokens/piece=296.421\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4086 obj=13.8507 num_tokens=1211180 num_tokens/piece=296.422\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3064 obj=14.477 num_tokens=1276819 num_tokens/piece=416.716\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3064 obj=14.3407 num_tokens=1278019 num_tokens/piece=417.108\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2298 obj=15.0081 num_tokens=1339851 num_tokens/piece=583.051\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2298 obj=14.8617 num_tokens=1339841 num_tokens/piece=583.047\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2200 obj=14.9505 num_tokens=1351816 num_tokens/piece=614.462\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2200 obj=14.9298 num_tokens=1351817 num_tokens/piece=614.462\n",
            "trainer_interface.cc(687) LOG(INFO) Saving model: m.model\n",
            "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab\n"
          ]
        }
      ],
      "source": [
        "spm.SentencePieceTrainer.train('--input=amharic.txt --vocab_size=2000 --model_prefix=m --pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3 --pad_piece=[PAD] --unk_piece=[UNK] --bos_piece=[BOS] --eos_piece=[EOS]')\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "\n",
        "for id in range(4):\n",
        "    print(sp.id_to_piece(id), sp.is_control(id))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPVnkpQstMOw"
      },
      "source": [
        "When -1 is set,  this special symbol is disabled. UNK must not be undefined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "59jHBemKlU8b",
        "outputId": "893ce083-7a31-468c-a49c-33d4ba962014"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n",
            "0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=amharic.txt --vocab_size=2000 --model_prefix=m --bos_id=-1 --eos_id=-1\n",
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: amharic.txt\n",
            "  input_format: \n",
            "  model_prefix: m\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 2000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: -1\n",
            "  eos_id: -1\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(185) LOG(INFO) Loading corpus: amharic.txt\n",
            "trainer_interface.cc(380) LOG(WARNING) Found too long line (5672 > 4192).\n",
            "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
            "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
            "trainer_interface.cc(409) LOG(INFO) Loaded all 39057 sentences\n",
            "trainer_interface.cc(416) LOG(INFO) Skipped 694 too long sentences.\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(539) LOG(INFO) all chars count=15040288\n",
            "trainer_interface.cc(550) LOG(INFO) Done: 99.9518% characters are covered.\n",
            "trainer_interface.cc(560) LOG(INFO) Alphabet size=237\n",
            "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999518\n",
            "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 38926 sentences.\n",
            "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=6788875\n",
            "unigram_model_trainer.cc(312) LOG(INFO) Initialized 377159 seed sentencepieces\n",
            "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 38926\n",
            "trainer_interface.cc(609) LOG(INFO) Done! 303267\n",
            "unigram_model_trainer.cc(602) LOG(INFO) Using 303267 sentences for EM training\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=148570 obj=12.6326 num_tokens=599116 num_tokens/piece=4.03255\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=129289 obj=11.0193 num_tokens=601387 num_tokens/piece=4.65149\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=96934 obj=11.0117 num_tokens=632315 num_tokens/piece=6.52315\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=96774 obj=10.986 num_tokens=632653 num_tokens/piece=6.53743\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=72579 obj=11.0866 num_tokens=674423 num_tokens/piece=9.29226\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=72571 obj=11.0592 num_tokens=674617 num_tokens/piece=9.29596\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=54427 obj=11.2001 num_tokens=719827 num_tokens/piece=13.2255\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=54426 obj=11.1668 num_tokens=719822 num_tokens/piece=13.2257\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=40819 obj=11.3491 num_tokens=766746 num_tokens/piece=18.784\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=40819 obj=11.3073 num_tokens=766725 num_tokens/piece=18.7835\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=30614 obj=11.5337 num_tokens=815166 num_tokens/piece=26.6272\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=30614 obj=11.4824 num_tokens=815184 num_tokens/piece=26.6278\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=22960 obj=11.7555 num_tokens=863615 num_tokens/piece=37.6139\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=22960 obj=11.6948 num_tokens=863636 num_tokens/piece=37.6148\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=17220 obj=12.0169 num_tokens=914727 num_tokens/piece=53.12\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=17220 obj=11.9466 num_tokens=914749 num_tokens/piece=53.1213\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12915 obj=12.3247 num_tokens=968463 num_tokens/piece=74.9875\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=12915 obj=12.2407 num_tokens=968487 num_tokens/piece=74.9893\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=9686 obj=12.6702 num_tokens=1024052 num_tokens/piece=105.725\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9686 obj=12.5759 num_tokens=1024266 num_tokens/piece=105.747\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=7264 obj=13.0632 num_tokens=1083459 num_tokens/piece=149.155\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=7264 obj=12.9563 num_tokens=1083497 num_tokens/piece=149.16\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5448 obj=13.4936 num_tokens=1144863 num_tokens/piece=210.144\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5448 obj=13.3756 num_tokens=1144902 num_tokens/piece=210.151\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4086 obj=13.979 num_tokens=1211177 num_tokens/piece=296.421\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4086 obj=13.8507 num_tokens=1211180 num_tokens/piece=296.422\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3064 obj=14.477 num_tokens=1276819 num_tokens/piece=416.716\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3064 obj=14.3407 num_tokens=1278019 num_tokens/piece=417.108\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2298 obj=15.0081 num_tokens=1339851 num_tokens/piece=583.051\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2298 obj=14.8617 num_tokens=1339841 num_tokens/piece=583.047\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2200 obj=14.9505 num_tokens=1351816 num_tokens/piece=614.462\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2200 obj=14.9298 num_tokens=1351817 num_tokens/piece=614.462\n",
            "trainer_interface.cc(687) LOG(INFO) Saving model: m.model\n",
            "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab\n"
          ]
        }
      ],
      "source": [
        "# Disable BOS/EOS\n",
        "spm.SentencePieceTrainer.train('--input=amharic.txt --vocab_size=2000 --model_prefix=m --bos_id=-1 --eos_id=-1')\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "# <s>, </s> are UNK.\n",
        "print(sp.unk_id())\n",
        "print(sp.piece_to_id('<s>'))\n",
        "print(sp.piece_to_id('</s>'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vDXA3Q6kjCS"
      },
      "source": [
        "## Sampling and nbest segmentation for subword regularization\n",
        "\n",
        "When **--model_type=unigram** (default) is used,  we can perform sampling and n-best segmentation for data augmentation. See subword regularization paper [[kudo18]](https://www.google.com/search?q=subword+regularization&rlz=1CAASUL_enJP841&oq=subword+regu&aqs=chrome.0.69i59j69i61j69i57j69i61l2j0.1571j0j7&sourceid=chrome&ie=UTF-8) for more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "nSQp93qflZO3",
        "outputId": "d5b45b62-2789-4879-b80b-f441e8b594af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁', 'በ', 'ኬ', 'ን', 'ያ', '▁', 'የ', 'ም', 'ር', 'ጫ', '▁ታሪክ']\n",
            "['▁በ', 'ኬንያ', '▁የምርጫ', '▁ታሪክ']\n",
            "['▁በ', 'ኬ', 'ን', 'ያ', '▁የ', 'ምርጫ', '▁ታሪክ']\n",
            "['▁በ', 'ኬ', 'ን', 'ያ', '▁የ', 'ምርጫ', '▁ታሪክ']\n",
            "['▁', 'በ', 'ኬ', 'ን', 'ያ', '▁የምርጫ', '▁ታሪክ']\n",
            "['▁በ', 'ኬንያ', '▁የምርጫ', '▁ታ', 'ሪ', 'ክ']\n",
            "['▁በ', 'ኬንያ', '▁የ', 'ምርጫ', '▁ታሪክ']\n",
            "['▁', 'በ', 'ኬንያ', '▁', 'የ', 'ምርጫ', '▁ታሪክ']\n",
            "['▁በ', 'ኬንያ', '▁የምርጫ', '▁ታሪክ']\n",
            "['▁', 'በ', 'ኬንያ', '▁', 'የ', 'ምርጫ', '▁ታሪክ']\n",
            "[8, 799, 1056, 1153]\n",
            "[8, 439, 5, 30, 0, 38, 1109, 1153]\n",
            "[8, 799, 6, 1109, 0, 23, 78, 40]\n",
            "[8, 799, 6, 9, 13, 139, 1153]\n",
            "[0, 18, 799, 1056, 1153]\n",
            "[8, 439, 5, 30, 1056, 211, 78, 40]\n",
            "[8, 799, 1056, 0, 23, 78, 40]\n",
            "[8, 439, 5, 30, 0, 38, 9, 13, 139, 1153]\n",
            "[0, 18, 799, 1056, 211, 78, 40]\n",
            "[8, 799, 1056, 1153]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=amharic.txt --model_prefix=m --vocab_size=2000\n",
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: amharic.txt\n",
            "  input_format: \n",
            "  model_prefix: m\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 2000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(185) LOG(INFO) Loading corpus: amharic.txt\n",
            "trainer_interface.cc(380) LOG(WARNING) Found too long line (5672 > 4192).\n",
            "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
            "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
            "trainer_interface.cc(409) LOG(INFO) Loaded all 39057 sentences\n",
            "trainer_interface.cc(416) LOG(INFO) Skipped 694 too long sentences.\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(539) LOG(INFO) all chars count=15040288\n",
            "trainer_interface.cc(550) LOG(INFO) Done: 99.9518% characters are covered.\n",
            "trainer_interface.cc(560) LOG(INFO) Alphabet size=237\n",
            "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999518\n",
            "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 38926 sentences.\n",
            "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=6788875\n",
            "unigram_model_trainer.cc(312) LOG(INFO) Initialized 377159 seed sentencepieces\n",
            "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 38926\n",
            "trainer_interface.cc(609) LOG(INFO) Done! 303267\n",
            "unigram_model_trainer.cc(602) LOG(INFO) Using 303267 sentences for EM training\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=148570 obj=12.6326 num_tokens=599116 num_tokens/piece=4.03255\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=129289 obj=11.0193 num_tokens=601387 num_tokens/piece=4.65149\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=96934 obj=11.0117 num_tokens=632315 num_tokens/piece=6.52315\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=96774 obj=10.986 num_tokens=632653 num_tokens/piece=6.53743\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=72579 obj=11.0866 num_tokens=674423 num_tokens/piece=9.29226\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=72571 obj=11.0592 num_tokens=674617 num_tokens/piece=9.29596\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=54427 obj=11.2001 num_tokens=719827 num_tokens/piece=13.2255\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=54426 obj=11.1668 num_tokens=719822 num_tokens/piece=13.2257\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=40819 obj=11.3491 num_tokens=766746 num_tokens/piece=18.784\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=40819 obj=11.3073 num_tokens=766725 num_tokens/piece=18.7835\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=30614 obj=11.5337 num_tokens=815166 num_tokens/piece=26.6272\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=30614 obj=11.4824 num_tokens=815184 num_tokens/piece=26.6278\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=22960 obj=11.7555 num_tokens=863615 num_tokens/piece=37.6139\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=22960 obj=11.6948 num_tokens=863636 num_tokens/piece=37.6148\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=17220 obj=12.0169 num_tokens=914727 num_tokens/piece=53.12\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=17220 obj=11.9466 num_tokens=914749 num_tokens/piece=53.1213\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12915 obj=12.3247 num_tokens=968463 num_tokens/piece=74.9875\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=12915 obj=12.2407 num_tokens=968487 num_tokens/piece=74.9893\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=9686 obj=12.6702 num_tokens=1024052 num_tokens/piece=105.725\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9686 obj=12.5759 num_tokens=1024266 num_tokens/piece=105.747\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=7264 obj=13.0632 num_tokens=1083459 num_tokens/piece=149.155\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=7264 obj=12.9563 num_tokens=1083497 num_tokens/piece=149.16\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5448 obj=13.4936 num_tokens=1144863 num_tokens/piece=210.144\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5448 obj=13.3756 num_tokens=1144902 num_tokens/piece=210.151\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4086 obj=13.979 num_tokens=1211177 num_tokens/piece=296.421\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4086 obj=13.8507 num_tokens=1211180 num_tokens/piece=296.422\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3064 obj=14.477 num_tokens=1276819 num_tokens/piece=416.716\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3064 obj=14.3407 num_tokens=1278019 num_tokens/piece=417.108\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2298 obj=15.0081 num_tokens=1339851 num_tokens/piece=583.051\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2298 obj=14.8617 num_tokens=1339841 num_tokens/piece=583.047\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2200 obj=14.9505 num_tokens=1351816 num_tokens/piece=614.462\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2200 obj=14.9298 num_tokens=1351817 num_tokens/piece=614.462\n",
            "trainer_interface.cc(687) LOG(INFO) Saving model: m.model\n",
            "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab\n"
          ]
        }
      ],
      "source": [
        "spm.SentencePieceTrainer.train('--input=amharic.txt --model_prefix=m --vocab_size=2000')\n",
        "\n",
        "# Can obtain different segmentations per request.\n",
        "# There are two hyperparameters for sampling (nbest_size and inverse temperature). see the paper [kudo18] for detail.\n",
        "for n in range(10):\n",
        "  print(sp.sample_encode_as_pieces('በኬንያ የምርጫ ታሪክ', -1, 0.1))\n",
        "\n",
        "for n in range(10):\n",
        "  print(sp.sample_encode_as_ids('በኬንያ የምርጫ ታሪክ', -1, 0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "9V1snUZdlb_v",
        "outputId": "bae2ca99-aca3-4013-c240-53b09a0bb684"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['▁በ', 'ኬንያ', '▁የምርጫ', '▁ታሪክ'], ['▁በ', 'ኬንያ', '▁የ', 'ምርጫ', '▁ታሪክ'], ['▁', 'በ', 'ኬንያ', '▁የምርጫ', '▁ታሪክ'], ['▁', 'በ', 'ኬንያ', '▁የ', 'ምርጫ', '▁ታሪክ'], ['▁በ', 'ኬ', 'ን', 'ያ', '▁የምርጫ', '▁ታሪክ'], ['▁በ', 'ኬንያ', '▁', 'የ', 'ምርጫ', '▁ታሪክ'], ['▁በ', 'ኬንያ', '▁የምርጫ', '▁ታ', 'ሪ', 'ክ'], ['▁በ', 'ኬንያ', '▁የ', 'ም', 'ር', 'ጫ', '▁ታሪክ'], ['▁በ', 'ኬንያ', '▁የምርጫ', '▁', 'ታ', 'ሪ', 'ክ'], ['▁በ', 'ኬ', 'ን', 'ያ', '▁የ', 'ምርጫ', '▁ታሪክ']]\n",
            "[[8, 799, 1056, 1153], [8, 799, 6, 1109, 1153], [0, 18, 799, 1056, 1153], [0, 18, 799, 6, 1109, 1153], [8, 439, 5, 30, 1056, 1153], [8, 799, 0, 38, 1109, 1153], [8, 799, 1056, 211, 78, 40], [8, 799, 6, 9, 13, 139, 1153], [8, 799, 1056, 0, 23, 78, 40], [8, 439, 5, 30, 6, 1109, 1153]]\n"
          ]
        }
      ],
      "source": [
        "# get 10 best\n",
        "print(sp.nbest_encode_as_pieces('በኬንያ የምርጫ ታሪክ', 10))\n",
        "print(sp.nbest_encode_as_ids('በኬንያ የምርጫ ታሪክ', 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH6cxuVNcDKh"
      },
      "source": [
        "## BPE (Byte pair encoding) model\n",
        "\n",
        "Sentencepiece supports BPE (byte-pair-encoding) for subword segmentation with **--model_type=bpe** flag.   We do not find empirical differences in translation quality between BPE and unigram model, but unigram model can perform sampling and n-best segmentation. See subword regularization paper [[kudo18]](https://www.google.com/search?q=subword+regularization&rlz=1CAASUL_enJP841&oq=subword+regu&aqs=chrome.0.69i59j69i61j69i57j69i61l2j0.1571j0j7&sourceid=chrome&ie=UTF-8) for more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "MNQxuX4Mc0KY",
        "outputId": "a6ed3e99-46c3-4c5e-dc8e-80394e17e363"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*** BPE ***\n",
            "['▁በ', 'ኬ', 'ንያ', '▁የምርጫ', '▁ታሪክ']\n",
            "[]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=amharic.txt --model_prefix=m_bpe --vocab_size=2000 --model_type=bpe\n",
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: amharic.txt\n",
            "  input_format: \n",
            "  model_prefix: m_bpe\n",
            "  model_type: BPE\n",
            "  vocab_size: 2000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(185) LOG(INFO) Loading corpus: amharic.txt\n",
            "trainer_interface.cc(380) LOG(WARNING) Found too long line (5672 > 4192).\n",
            "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
            "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
            "trainer_interface.cc(409) LOG(INFO) Loaded all 39057 sentences\n",
            "trainer_interface.cc(416) LOG(INFO) Skipped 694 too long sentences.\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(539) LOG(INFO) all chars count=15040288\n",
            "trainer_interface.cc(550) LOG(INFO) Done: 99.9518% characters are covered.\n",
            "trainer_interface.cc(560) LOG(INFO) Alphabet size=237\n",
            "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999518\n",
            "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 38926 sentences.\n",
            "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 38926\n",
            "trainer_interface.cc(609) LOG(INFO) Done! 303267\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=332390 min_freq=2172\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=41139 size=20 all=25328 active=4110 piece=▁ማ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25561 size=40 all=27713 active=6495 piece=ትዮ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19899 size=60 all=29821 active=8603 piece=▁ድ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15996 size=80 all=32058 active=10840 piece=ንድ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12425 size=100 all=33847 active=12629 piece=ዲስ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=12346 min_freq=1250\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10906 size=120 all=35680 active=3450 piece=▁ምክ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9653 size=140 all=37628 active=5398 piece=ረጃ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8947 size=160 all=39142 active=6912 piece=ከተ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8346 size=180 all=40785 active=8555 piece=▁የነ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7831 size=200 all=42477 active=10247 piece=▁ች\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7819 min_freq=925\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7124 size=220 all=44198 active=3818 piece=▁መል\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6743 size=240 all=46118 active=5738 piece=ባቸው\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6228 size=260 all=47687 active=7307 piece=ተና\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5928 size=280 all=49318 active=8938 piece=▁አደ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5556 size=300 all=50998 active=10618 piece=▁በላይ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5548 min_freq=718\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5281 size=320 all=52200 active=3725 piece=▁መንግስት\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5086 size=340 all=53199 active=4724 piece=ይት\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4793 size=360 all=54443 active=5968 piece=▁ዜ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4481 size=380 all=56176 active=7701 piece=▁አድር\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4265 size=400 all=57443 active=8968 piece=ታቸውን\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4264 min_freq=594\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4119 size=420 all=58550 active=3851 piece=ሀገ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3977 size=440 all=59445 active=4746 piece=▁አስታው\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3770 size=460 all=60963 active=6264 piece=▁ኃላፊ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3523 size=480 all=62091 active=7392 piece=▁ፈተና\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3374 size=500 all=63520 active=8821 piece=ጠቀ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3368 min_freq=502\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3269 size=520 all=64683 active=4235 piece=▁ገልፀ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3209 size=540 all=66007 active=5559 piece=ብራ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3088 size=560 all=67117 active=6669 piece=ከሰ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2990 size=580 all=68355 active=7907 piece=▁ምክር\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2894 size=600 all=69405 active=8957 piece=ችል\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2889 min_freq=435\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2798 size=620 all=70727 active=4721 piece=ንክ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2712 size=640 all=71831 active=5825 piece=▁ከዚህ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2596 size=660 all=73067 active=7061 piece=በል\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2507 size=680 all=74150 active=8144 piece=በአ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2442 size=700 all=75362 active=9356 piece=ኤል\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2441 min_freq=381\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2381 size=720 all=76195 active=4500 piece=ዳማ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2328 size=740 all=77279 active=5584 piece=ቀጥ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2251 size=760 all=78126 active=6431 piece=ትር\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2198 size=780 all=79369 active=7674 piece=ሱን\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2154 size=800 all=80517 active=8822 piece=▁አድ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2153 min_freq=340\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2104 size=820 all=81570 active=5023 piece=▁ብሔ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2046 size=840 all=82266 active=5719 piece=ነሳ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2009 size=860 all=83460 active=6913 piece=ላቸውን\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1959 size=880 all=84374 active=7827 piece=ድሞ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1919 size=900 all=85462 active=8915 piece=ምባ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1919 min_freq=306\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1883 size=920 all=86395 active=5094 piece=▁እጅ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1838 size=940 all=87595 active=6294 piece=ሯል።\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1797 size=960 all=88780 active=7479 piece=▁ክት\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1762 size=980 all=89654 active=8353 piece=▁የከተማ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1714 size=1000 all=90281 active=8980 piece=▁ዙሪያ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1712 min_freq=278\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1683 size=1020 all=91309 active=5528 piece=▁ዋጋ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1655 size=1040 all=92300 active=6519 piece=ዕራ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1626 size=1060 all=93183 active=7402 piece=በትን\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1595 size=1080 all=94051 active=8270 piece=ረን\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1555 size=1100 all=95125 active=9344 piece=ህበራዊ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1554 min_freq=255\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1525 size=1120 all=96181 active=5802 piece=ልቅ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1497 size=1140 all=96977 active=6598 piece=ስቲያን\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1473 size=1160 all=97899 active=7520 piece=▁አመት\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1447 size=1180 all=98959 active=8580 piece=ወቅ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1424 size=1200 all=99890 active=9511 piece=▁ተወካ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1422 min_freq=236\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1390 size=1220 all=100870 active=5966 piece=▁ከተሞች\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1369 size=1240 all=102008 active=7104 piece=ጥብ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1347 size=1260 all=103386 active=8482 piece=ወር\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1323 size=1280 all=104726 active=9822 piece=ጠሩ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1298 size=1300 all=105651 active=10747 piece=▁የሚል\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1297 min_freq=217\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1280 size=1320 all=106505 active=6123 piece=ንግድ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1256 size=1340 all=107641 active=7259 piece=▁ዳይሬክተር\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1239 size=1360 all=108624 active=8242 piece=ደርስ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1221 size=1380 all=109223 active=8841 piece=▁ከላይ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1194 size=1400 all=110088 active=9706 piece=ጀር\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1192 min_freq=203\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1177 size=1420 all=110848 active=6196 piece=ታይ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1155 size=1440 all=111719 active=7067 piece=▁ሱዳን\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1135 size=1460 all=112692 active=8040 piece=▁ተያይ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1112 size=1480 all=113418 active=8766 piece=ጠረ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1087 size=1500 all=114253 active=9601 piece=▁ሰራተኞች\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1086 min_freq=190\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1073 size=1520 all=114978 active=6428 piece=ፌስቡ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1059 size=1540 all=115362 active=6812 piece=▁መስተዳ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1041 size=1560 all=116139 active=7589 piece=▁ተሰም\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1028 size=1580 all=117202 active=8652 piece=▁ትኩረት\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1015 size=1600 all=118142 active=9592 piece=▁ንግግር\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1015 min_freq=179\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1003 size=1620 all=119089 active=6840 piece=▁የቀረ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=986 size=1640 all=120069 active=7820 piece=▁የንግድ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=970 size=1660 all=120900 active=8651 piece=ኮኖ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=960 size=1680 all=121597 active=9348 piece=▁በማለት\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=948 size=1700 all=122466 active=10217 piece=ታማ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=948 min_freq=169\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=931 size=1720 all=123305 active=6892 piece=▁ቅሬ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=917 size=1740 all=123903 active=7490 piece=▁በባ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=900 size=1760 all=124758 active=8345 piece=▁ለእ\n",
            "trainer_interface.cc(687) LOG(INFO) Saving model: m_bpe.model\n",
            "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m_bpe.vocab\n"
          ]
        }
      ],
      "source": [
        "spm.SentencePieceTrainer.train('--input=amharic.txt --model_prefix=m_bpe --vocab_size=2000 --model_type=bpe')\n",
        "sp_bpe = spm.SentencePieceProcessor()\n",
        "sp_bpe.load('m_bpe.model')\n",
        "\n",
        "print('*** BPE ***')\n",
        "print(sp_bpe.encode_as_pieces('በኬንያ የምርጫ ታሪክ'))\n",
        "print(sp_bpe.nbest_encode_as_pieces('hello world', 5))  # returns an empty list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "EZrj1zCkvK8v",
        "outputId": "8d5421d9-9b89-440d-c91a-f03c081947d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*** Unigram ***\n",
            "['▁በ', 'ኬንያ', '▁የምርጫ', '▁ታሪክ']\n",
            "[['▁በ', 'ኬንያ', '▁የምርጫ', '▁ታሪክ'], ['▁በ', 'ኬንያ', '▁የ', 'ምርጫ', '▁ታሪክ'], ['▁', 'በ', 'ኬንያ', '▁የምርጫ', '▁ታሪክ'], ['▁', 'በ', 'ኬንያ', '▁የ', 'ምርጫ', '▁ታሪክ'], ['▁በ', 'ኬ', 'ን', 'ያ', '▁የምርጫ', '▁ታሪክ']]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=amharic.txt --model_prefix=m_unigram --vocab_size=2000 --model_type=unigram\n",
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: amharic.txt\n",
            "  input_format: \n",
            "  model_prefix: m_unigram\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 2000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(185) LOG(INFO) Loading corpus: amharic.txt\n",
            "trainer_interface.cc(380) LOG(WARNING) Found too long line (5672 > 4192).\n",
            "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
            "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
            "trainer_interface.cc(409) LOG(INFO) Loaded all 39057 sentences\n",
            "trainer_interface.cc(416) LOG(INFO) Skipped 694 too long sentences.\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(539) LOG(INFO) all chars count=15040288\n",
            "trainer_interface.cc(550) LOG(INFO) Done: 99.9518% characters are covered.\n",
            "trainer_interface.cc(560) LOG(INFO) Alphabet size=237\n",
            "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999518\n",
            "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 38926 sentences.\n",
            "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=6788875\n",
            "unigram_model_trainer.cc(312) LOG(INFO) Initialized 377159 seed sentencepieces\n",
            "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 38926\n",
            "trainer_interface.cc(609) LOG(INFO) Done! 303267\n",
            "unigram_model_trainer.cc(602) LOG(INFO) Using 303267 sentences for EM training\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=148570 obj=12.6326 num_tokens=599116 num_tokens/piece=4.03255\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=129289 obj=11.0193 num_tokens=601387 num_tokens/piece=4.65149\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=96934 obj=11.0117 num_tokens=632315 num_tokens/piece=6.52315\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=96774 obj=10.986 num_tokens=632653 num_tokens/piece=6.53743\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=72579 obj=11.0866 num_tokens=674423 num_tokens/piece=9.29226\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=72571 obj=11.0592 num_tokens=674617 num_tokens/piece=9.29596\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=54427 obj=11.2001 num_tokens=719827 num_tokens/piece=13.2255\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=54426 obj=11.1668 num_tokens=719822 num_tokens/piece=13.2257\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=40819 obj=11.3491 num_tokens=766746 num_tokens/piece=18.784\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=40819 obj=11.3073 num_tokens=766725 num_tokens/piece=18.7835\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=30614 obj=11.5337 num_tokens=815166 num_tokens/piece=26.6272\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=30614 obj=11.4824 num_tokens=815184 num_tokens/piece=26.6278\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=22960 obj=11.7555 num_tokens=863615 num_tokens/piece=37.6139\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=22960 obj=11.6948 num_tokens=863636 num_tokens/piece=37.6148\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=17220 obj=12.0169 num_tokens=914727 num_tokens/piece=53.12\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=17220 obj=11.9466 num_tokens=914749 num_tokens/piece=53.1213\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12915 obj=12.3247 num_tokens=968463 num_tokens/piece=74.9875\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=12915 obj=12.2407 num_tokens=968487 num_tokens/piece=74.9893\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=9686 obj=12.6702 num_tokens=1024052 num_tokens/piece=105.725\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9686 obj=12.5759 num_tokens=1024266 num_tokens/piece=105.747\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=7264 obj=13.0632 num_tokens=1083459 num_tokens/piece=149.155\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=7264 obj=12.9563 num_tokens=1083497 num_tokens/piece=149.16\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5448 obj=13.4936 num_tokens=1144863 num_tokens/piece=210.144\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5448 obj=13.3756 num_tokens=1144902 num_tokens/piece=210.151\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4086 obj=13.979 num_tokens=1211177 num_tokens/piece=296.421\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4086 obj=13.8507 num_tokens=1211180 num_tokens/piece=296.422\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3064 obj=14.477 num_tokens=1276819 num_tokens/piece=416.716\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3064 obj=14.3407 num_tokens=1278019 num_tokens/piece=417.108\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2298 obj=15.0081 num_tokens=1339851 num_tokens/piece=583.051\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2298 obj=14.8617 num_tokens=1339841 num_tokens/piece=583.047\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2200 obj=14.9505 num_tokens=1351816 num_tokens/piece=614.462\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2200 obj=14.9298 num_tokens=1351817 num_tokens/piece=614.462\n",
            "trainer_interface.cc(687) LOG(INFO) Saving model: m_unigram.model\n",
            "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m_unigram.vocab\n"
          ]
        }
      ],
      "source": [
        "spm.SentencePieceTrainer.train('--input=amharic.txt --model_prefix=m_unigram --vocab_size=2000 --model_type=unigram')\n",
        "sp_unigram = spm.SentencePieceProcessor()\n",
        "sp_unigram.load('m_unigram.model')\n",
        "\n",
        "print('*** Unigram ***')\n",
        "print(sp_unigram.encode_as_pieces('በኬንያ የምርጫ ታሪክ'))\n",
        "print(sp_unigram.nbest_encode_as_pieces('በኬንያ የምርጫ ታሪክ', 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJXHCoAHoZWg"
      },
      "source": [
        "## Character and word model\n",
        "\n",
        "Sentencepiece supports character and word segmentation with **--model_type=char** and **--model_type=character** flags.\n",
        "\n",
        "In `word` segmentation, sentencepiece just segments tokens with whitespaces, so the input text must be pre-tokenized.\n",
        "We can apply different segmentation algorithms transparently without changing pre/post processors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "pOAOmQGQpBhg",
        "outputId": "0bcfa075-4231-4299-e117-c4d02dae0872"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁', 'በ', 'ኬ', 'ን', 'ያ', '▁', 'የ', 'ም', 'ር', 'ጫ', '▁', 'ታ', 'ሪ', 'ክ', '▁', 'ለ', 'መ', 'ጀ', 'መ', 'ሪ', 'ያ', '▁', 'ጊ', 'ዜ', '▁', 'የ', 'ህ', 'ግ', '▁', 'ታ', 'ራ', 'ሚ', 'ዎ', 'ች', '▁', 'ድ', 'ም', 'ጽ', '▁', 'ሰ', 'ጡ', '፡', '፡']\n",
            "[3, 7, 177, 4, 18, 3, 6, 15, 9, 118, 3, 36, 44, 38, 3, 16, 12, 92, 12, 44, 18, 3, 128, 110, 3, 6, 40, 28, 3, 36, 33, 37, 49, 20, 3, 34, 15, 168, 3, 29, 140, 60, 60]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=amharic.txt --model_prefix=m_char --model_type=char --vocab_size=2000\n",
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: amharic.txt\n",
            "  input_format: \n",
            "  model_prefix: m_char\n",
            "  model_type: CHAR\n",
            "  vocab_size: 2000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(185) LOG(INFO) Loading corpus: amharic.txt\n",
            "trainer_interface.cc(380) LOG(WARNING) Found too long line (5672 > 4192).\n",
            "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
            "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
            "trainer_interface.cc(409) LOG(INFO) Loaded all 39057 sentences\n",
            "trainer_interface.cc(416) LOG(INFO) Skipped 694 too long sentences.\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(539) LOG(INFO) all chars count=15040288\n",
            "trainer_interface.cc(550) LOG(INFO) Done: 99.9518% characters are covered.\n",
            "trainer_interface.cc(560) LOG(INFO) Alphabet size=237\n",
            "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999518\n",
            "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 38926 sentences.\n",
            "trainer_interface.cc(687) LOG(INFO) Saving model: m_char.model\n",
            "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m_char.vocab\n"
          ]
        }
      ],
      "source": [
        "spm.SentencePieceTrainer.train('--input=amharic.txt --model_prefix=m_char --model_type=char --vocab_size=2000')\n",
        "\n",
        "sp_char = spm.SentencePieceProcessor()\n",
        "sp_char.load('m_char.model')\n",
        "\n",
        "print(sp_char.encode_as_pieces('በኬንያ የምርጫ ታሪክ ለመጀመሪያ ጊዜ የህግ ታራሚዎች ድምጽ ሰጡ፡፡'))\n",
        "print(sp_char.encode_as_ids('በኬንያ የምርጫ ታሪክ ለመጀመሪያ ጊዜ የህግ ታራሚዎች ድምጽ ሰጡ፡፡'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "uzBiPAm4ljor",
        "outputId": "299083c5-e453-4a4b-86eb-c1abd0aa34e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁በኬንያ', '▁የምርጫ', '▁ታሪክ', '▁ለመጀመሪያ', '▁ጊዜ', '▁የህግ', '▁ታራሚዎች', '▁ድምጽ', '▁ሰጡ፡፡']\n",
            "[1559, 338, 427, 1323, 25, 630, 0, 900, 0]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=amharic.txt --model_prefix=m_word --model_type=word --vocab_size=2000\n",
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: amharic.txt\n",
            "  input_format: \n",
            "  model_prefix: m_word\n",
            "  model_type: WORD\n",
            "  vocab_size: 2000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(185) LOG(INFO) Loading corpus: amharic.txt\n",
            "trainer_interface.cc(380) LOG(WARNING) Found too long line (5672 > 4192).\n",
            "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
            "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
            "trainer_interface.cc(409) LOG(INFO) Loaded all 39057 sentences\n",
            "trainer_interface.cc(416) LOG(INFO) Skipped 694 too long sentences.\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(539) LOG(INFO) all chars count=15040288\n",
            "trainer_interface.cc(550) LOG(INFO) Done: 99.9518% characters are covered.\n",
            "trainer_interface.cc(560) LOG(INFO) Alphabet size=237\n",
            "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999518\n",
            "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 38926 sentences.\n",
            "trainer_interface.cc(687) LOG(INFO) Saving model: m_word.model\n",
            "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m_word.vocab\n"
          ]
        }
      ],
      "source": [
        "spm.SentencePieceTrainer.train('--input=amharic.txt --model_prefix=m_word --model_type=word --vocab_size=2000')\n",
        "\n",
        "sp_word = spm.SentencePieceProcessor()\n",
        "sp_word.load('m_word.model')\n",
        "\n",
        "print(sp_word.encode_as_pieces('በኬንያ የምርጫ ታሪክ ለመጀመሪያ ጊዜ የህግ ታራሚዎች ድምጽ ሰጡ፡፡'))  # '.' will not be one token.\n",
        "print(sp_word.encode_as_ids('በኬንያ የምርጫ ታሪክ ለመጀመሪያ ጊዜ የህግ ታራሚዎች ድምጽ ሰጡ፡፡'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZvkFnw9pt-D"
      },
      "source": [
        "## Text normalization\n",
        "\n",
        "Sentencepiece provides the following general pre-defined normalization rules. We can change the normalizer with **--normaliation_rule_name=&lt;NAME&gt;** flag.\n",
        "\n",
        "- **nmt_nfkc**: NFKC normalization with some additional normalization around spaces. (default)\n",
        "- **nfkc: original**: NFKC normalization.\n",
        "- **nmt_nfkc_cf**: nmt_nfkc + Unicode case folding (mostly lower casing)\n",
        "- **nfkc_cf**: nfkc + Unicode case folding.\n",
        "- **identity**: no normalization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "jSJiwIeBqFcO",
        "outputId": "c35ec700-dbb9-4601-9d00-6c04a5eef147"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁what', '▁else', '▁is', '▁there', '▁', 'hello', '▁world', '.']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=botchan.txt --model_prefix=m --vocab_size=2000 --normalization_rule_name=nfkc_cf\n",
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: botchan.txt\n",
            "  input_format: \n",
            "  model_prefix: m\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 2000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nfkc_cf\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(185) LOG(INFO) Loading corpus: botchan.txt\n",
            "trainer_interface.cc(409) LOG(INFO) Loaded all 4288 sentences\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(539) LOG(INFO) all chars count=278541\n",
            "trainer_interface.cc(550) LOG(INFO) Done: 99.953% characters are covered.\n",
            "trainer_interface.cc(560) LOG(INFO) Alphabet size=46\n",
            "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.99953\n",
            "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 4288 sentences.\n",
            "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=148630\n",
            "unigram_model_trainer.cc(312) LOG(INFO) Initialized 15490 seed sentencepieces\n",
            "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 4288\n",
            "trainer_interface.cc(609) LOG(INFO) Done! 9777\n",
            "unigram_model_trainer.cc(602) LOG(INFO) Using 9777 sentences for EM training\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5415 obj=10.7365 num_tokens=21107 num_tokens/piece=3.89788\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4766 obj=8.66585 num_tokens=21184 num_tokens/piece=4.44482\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3574 obj=8.7301 num_tokens=22606 num_tokens/piece=6.32513\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3573 obj=8.67714 num_tokens=22607 num_tokens/piece=6.32718\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2679 obj=8.9678 num_tokens=24941 num_tokens/piece=9.30982\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2679 obj=8.89403 num_tokens=24942 num_tokens/piece=9.31019\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2200 obj=9.11861 num_tokens=26698 num_tokens/piece=12.1355\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2200 obj=9.06818 num_tokens=26700 num_tokens/piece=12.1364\n",
            "trainer_interface.cc(687) LOG(INFO) Saving model: m.model\n",
            "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab\n",
            "] contains escaped characters that break the format of m.vocab\n"
          ]
        }
      ],
      "source": [
        "# NFKC normalization and lower casing.\n",
        "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m --vocab_size=2000 --normalization_rule_name=nfkc_cf')\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "print(sp.encode_as_pieces('ＨＥＬＬＯ　ＷＯＲＬＤ.'))  # lower casing and normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp1QiTjprER4"
      },
      "source": [
        "The normalization is performed with user-defined string-to-string mappings and leftmost longest matching.\n",
        "We can also define the custom normalization rules as TSV file. The TSV files for pre-defined normalization rules can be found in the data directory ([sample](https://raw.githubusercontent.com/google/sentencepiece/master/data/nfkc.tsv)). The normalization rule is compiled into FST and embedded in the model file. We don't need to specify the normalization configuration in the segmentation phase.\n",
        "\n",
        "Here's the example of custom normalization. The TSV file is fed with **--normalization_rule_tsv=&lt;FILE&gt;** flag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "xHM5aGYTrfXg",
        "outputId": "b9652d0d-6e03-486b-fe69-710153e4c906"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "U+49 U+27 U+6d\tU+49 U+20 U+61 U+6d\n",
            "U+64 U+6f U+6e U+27 U+74\tU+64 U+6f U+20 U+6e U+6f U+74\n",
            "\n",
            "['▁I', '▁am', '▁bu', 's', 'y']\n",
            "['▁I', '▁do', '▁not', '▁know', '▁it', '.']\n"
          ]
        }
      ],
      "source": [
        "def tocode(s):\n",
        "    out = []\n",
        "    for c in s:\n",
        "        out.append(str(hex(ord(c))).replace('0x', 'U+'))\n",
        "    return ' '.join(out)\n",
        "\n",
        "\n",
        "# TSV format:  source Unicode code points <tab> target code points\n",
        "# normalize \"don't => do not,  I'm => I am\"\n",
        "with open('normalization_rule.tsv', 'w') as f:\n",
        "  f.write(tocode(\"I'm\") + '\\t' + tocode(\"I am\") + '\\n')\n",
        "  f.write(tocode(\"don't\") + '\\t' + tocode(\"do not\") + '\\n')\n",
        "\n",
        "print(open('normalization_rule.tsv', 'r').read())\n",
        "\n",
        "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m --vocab_size=2000 --normalization_rule_tsv=normalization_rule.tsv')\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "# m.model embeds the normalization rule compiled into an FST.\n",
        "sp.load('m.model')\n",
        "print(sp.encode_as_pieces(\"I'm busy\"))  # normalized to `I am busy'\n",
        "print(sp.encode_as_pieces(\"I don't know it.\"))  # normalized to 'I do not know it.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdSx1bizvSbH"
      },
      "source": [
        "## Randomizing training data\n",
        "\n",
        "Sentencepiece loads all the lines of training data into memory to train the model.  However, larger training data increases the training time and memory usage, though they are linear to the training data. When **--input_sentence_size=&lt;SIZE&gt;** is specified,  Sentencepiece randomly samples &lt;SIZE&gt; lines from the whole training data.   **--shuffle_input_sentence=false** disables the random shuffle and takes the first &lt;SIZE&gt; lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "FZ089HOXwppS",
        "outputId": "607ec114-f443-44e1-ce88-002b5e6deb36"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=botchan.txt --model_prefix=m --vocab_size=2000 --input_sentence_size=1000\n",
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: botchan.txt\n",
            "  input_format: \n",
            "  model_prefix: m\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 2000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 1000\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(185) LOG(INFO) Loading corpus: botchan.txt\n",
            "trainer_interface.cc(411) LOG(INFO) Sampled 1000 sentences from 4288 sentences.\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(539) LOG(INFO) all chars count=63394\n",
            "trainer_interface.cc(550) LOG(INFO) Done: 99.9543% characters are covered.\n",
            "trainer_interface.cc(560) LOG(INFO) Alphabet size=67\n",
            "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999543\n",
            "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 1000 sentences.\n",
            "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=32240\n",
            "unigram_model_trainer.cc(312) LOG(INFO) Initialized 6515 seed sentencepieces\n",
            "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 1000\n",
            "trainer_interface.cc(609) LOG(INFO) Done! 3471\n",
            "unigram_model_trainer.cc(602) LOG(INFO) Using 3471 sentences for EM training\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2683 obj=10.963 num_tokens=7482 num_tokens/piece=2.78867\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2398 obj=9.35783 num_tokens=7517 num_tokens/piece=3.1347\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2199 obj=9.31094 num_tokens=7631 num_tokens/piece=3.47021\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2184 obj=9.28202 num_tokens=7633 num_tokens/piece=3.49496\n",
            "trainer_interface.cc(687) LOG(INFO) Saving model: m.model\n",
            "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['▁this', '▁is', '▁a', '▁t', 'est', '.']"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m --vocab_size=2000 --input_sentence_size=1000')\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "sp.encode_as_pieces('this is a test.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07FMNoCmglil"
      },
      "source": [
        "## Vocabulary restriction\n",
        "\n",
        "We can encode the text only using the tokens specified with **set_vocabulary** method.  The background of this feature is described in [subword-nmt page](https://github.com/rsennrich/subword-nmt#best-practice-advice-for-byte-pair-encoding-in-nmt)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2soU1eZhdH_"
      },
      "outputs": [],
      "source": [
        "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m --vocab_size=2000')\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "print(sp.encode_as_pieces('this is a test.'))\n",
        "\n",
        "# Gets all tokens as Python list.\n",
        "vocabs = [sp.id_to_piece(id) for id in range(sp.get_piece_size())]\n",
        "\n",
        "# Aggregates the frequency of each token in the training data.\n",
        "freq = {}\n",
        "with open('botchan.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.rstrip()\n",
        "        for piece in sp.encode_as_pieces(line):\n",
        "            freq.setdefault(piece, 0)\n",
        "            freq[piece] += 1\n",
        "\n",
        "# only uses the token appearing more than 1000 times in the training data.\n",
        "vocabs = list(filter(lambda x: x in freq and freq[x] > 1000, vocabs))\n",
        "sp.set_vocabulary(vocabs)\n",
        "print(sp.encode_as_pieces('this is a test.'))\n",
        "\n",
        "# reset the restriction\n",
        "sp.reset_vocabulary()\n",
        "print(sp.encode_as_pieces('this is a test.'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8rQLqCTHk40"
      },
      "source": [
        "## Extracting crossing-words pieces\n",
        "\n",
        "Sentencepieces does not extract pieces crossing multiple words (here the `word` means the space delimited tokens). The piece will never contain the whitespace marker (_) in the middle.\n",
        "\n",
        "**--split_by_whtespace=false** disables this restriction and allows to extract pieces crossing multiple words.  In CJK (Chinese/Japanese/Korean), this flag will not affect the final segmentation results so much as  words are not tokenized with whitespaces in CJK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "Lf5Fs_pPIKif",
        "outputId": "fc1cff92-bb97-4ab9-c52a-8e3c47d1d88f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ed▁to\n",
            "s▁of\n",
            "ing▁the\n",
            "ed▁the\n",
            "s▁and\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=botchan.txt --model_prefix=m --vocab_size=2000 --split_by_whitespace=false\n",
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: botchan.txt\n",
            "  input_format: \n",
            "  model_prefix: m\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 2000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 0\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(185) LOG(INFO) Loading corpus: botchan.txt\n",
            "trainer_interface.cc(409) LOG(INFO) Loaded all 4288 sentences\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(539) LOG(INFO) all chars count=274252\n",
            "trainer_interface.cc(550) LOG(INFO) Done: 99.957% characters are covered.\n",
            "trainer_interface.cc(560) LOG(INFO) Alphabet size=69\n",
            "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.99957\n",
            "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 4288 sentences.\n",
            "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=144687\n",
            "unigram_model_trainer.cc(312) LOG(INFO) Initialized 71445 seed sentencepieces\n",
            "unigram_model_trainer.cc(602) LOG(INFO) Using 4288 sentences for EM training\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=14935 obj=110.494 num_tokens=48648 num_tokens/piece=3.25732\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=12036 obj=92.8009 num_tokens=49242 num_tokens/piece=4.09123\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8905 obj=92.9673 num_tokens=51819 num_tokens/piece=5.81909\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8752 obj=91.8375 num_tokens=51971 num_tokens/piece=5.93819\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6509 obj=94.6287 num_tokens=55150 num_tokens/piece=8.47288\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6457 obj=93.5591 num_tokens=55179 num_tokens/piece=8.54561\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4829 obj=97.212 num_tokens=58812 num_tokens/piece=12.1789\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4819 obj=96.1527 num_tokens=58825 num_tokens/piece=12.2069\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3613 obj=100.556 num_tokens=63036 num_tokens/piece=17.447\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3611 obj=99.4715 num_tokens=63042 num_tokens/piece=17.4583\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2708 obj=104.499 num_tokens=67792 num_tokens/piece=25.034\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2707 obj=103.381 num_tokens=67793 num_tokens/piece=25.0436\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2200 obj=107.273 num_tokens=71408 num_tokens/piece=32.4582\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2200 obj=106.506 num_tokens=71409 num_tokens/piece=32.4586\n",
            "trainer_interface.cc(687) LOG(INFO) Saving model: m.model\n",
            "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m --vocab_size=2000 --split_by_whitespace=false')\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "# Gets all tokens as Python list.\n",
        "vocabs = [sp.id_to_piece(id) for id in range(sp.get_piece_size())]\n",
        "\n",
        "for piece in vocabs[0:500]:\n",
        "    if re.match('\\w+▁\\w+', piece):\n",
        "        print(piece)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWjA7yOX1Rlg"
      },
      "source": [
        "## Training sentencepiece model from the word list with frequency\n",
        "\n",
        "We can train the sentencepiece model from the pair of &lt;word, frequency&gt;. First, you make a TSV file where the first column is the word and the second column is the frequency. Then, feed this TSV file with **--input_format=tsv** flag. Note that when feeding TSV as training data, we implicitly assume that **--split_by_whtespace=true**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "T7F349Sd2Bzg",
        "outputId": "3453d1f2-2614-4258-ed7d-5a2e230b29ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁this', '▁is', '▁a', '▁t', 'est', '.']\n"
          ]
        }
      ],
      "source": [
        "freq = {}\n",
        "with open('botchan.txt', 'r') as f:\n",
        "  for line in f:\n",
        "    line = line.rstrip()\n",
        "    for piece in line.split():\n",
        "      freq.setdefault(piece, 0)\n",
        "      freq[piece] += 1\n",
        "\n",
        "with open('word_freq_list.tsv', 'w') as f:\n",
        "  for k, v in freq.items():\n",
        "    f.write('%s\\t%d\\n' % (k, v))\n",
        "\n",
        "spm.SentencePieceTrainer.train('--input=word_freq_list.tsv --input_format=tsv --model_prefix=m --vocab_size=2000')\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "print(sp.encode_as_pieces('this is a test.'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiWMoTpA-pHx"
      },
      "source": [
        "## Getting byte offsets of tokens\n",
        "\n",
        "Sentencepiece keeps track of byte offset (span) of each token, which is useful for highlighting the token on top of unnormalized text.\n",
        "\n",
        "We first need to install protobuf module as the byte offsets and all other meta data for segementation are encoded in protocol buffer.\n",
        "**encode_as_serialized_proto** method returns serialized SentencePieceText proto. You can get the deserialized object by calling ParseFromString method.\n",
        "\n",
        "The definition of SentencePieceText proto is found [here](https://github.com/google/sentencepiece/blob/3be3f2e11e2bb923c579c6be5e7335809341587f/src/sentencepiece.proto#L23).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "JTYrvL6KkmVK",
        "outputId": "1459a127-7aed-4296-f8e6-0c5e76c6c3d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (3.7.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf) (1.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf) (40.8.0)\n",
            "--2019-03-27 21:42:35--  https://raw.githubusercontent.com/google/sentencepiece/master/python/sentencepiece_pb2.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7382 (7.2K) [text/plain]\n",
            "Saving to: ‘sentencepiece_pb2.py.1’\n",
            "\n",
            "sentencepiece_pb2.p 100%[===================>]   7.21K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-03-27 21:42:35 (52.3 MB/s) - ‘sentencepiece_pb2.py.1’ saved [7382/7382]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install protobuf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "KdRy9sEvk7zw",
        "outputId": "80b1a4e5-8cbb-46bc-9549-e24444328f79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text: \"\\357\\275\\210\\357\\275\\205\\357\\275\\214\\357\\275\\214\\357\\275\\217\"\n",
            "pieces {\n",
            "  piece: \"\\342\\226\\201he\"\n",
            "  id: 28\n",
            "  surface: \"\\357\\275\\210\\357\\275\\205\"\n",
            "  begin: 0\n",
            "  end: 6\n",
            "}\n",
            "pieces {\n",
            "  piece: \"ll\"\n",
            "  id: 98\n",
            "  surface: \"\\357\\275\\214\\357\\275\\214\"\n",
            "  begin: 6\n",
            "  end: 12\n",
            "}\n",
            "pieces {\n",
            "  piece: \"o\"\n",
            "  id: 38\n",
            "  surface: \"\\357\\275\\217\"\n",
            "  begin: 12\n",
            "  end: 15\n",
            "}\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "489"
            ]
          },
          "execution_count": 50,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sentencepiece import sentencepiece_pb2\n",
        "\n",
        "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m --vocab_size=2000')\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "# One best result\n",
        "spt = sentencepiece_pb2.SentencePieceText()\n",
        "spt.ParseFromString(sp.encode_as_serialized_proto('ｈｅｌｌｏ')) # Full width hello\n",
        "\n",
        "# begin/end (offsets) are pointing to the original input.\n",
        "print(spt)\n",
        "\n",
        "# Nbest results\n",
        "nspt = sentencepiece_pb2.NBestSentencePieceText()\n",
        "nspt.ParseFromString(sp.nbest_encode_as_serialized_proto('ｈｅｌｌｏ', 5))\n",
        "# print(nspt)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
